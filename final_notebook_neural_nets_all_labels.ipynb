{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#for reading in data properly\n",
    "import ast\n",
    "import json\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, LSTM, SimpleRNN, Dense, Dropout, Flatten, Bidirectional\n",
    "from keras.layers import Input, concatenate, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('train.csv')\n",
    "all_data = all_data.dropna(subset=['overview', 'genres']) #drop cols without overview or genre (data we use or labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse each row to get label vectors from json\n",
    "def parse_genres_json(x):\n",
    "    try:\n",
    "        json_genres = json.loads(x.replace(\"'\", '\"'))\n",
    "        numElems = len(json_genres)\n",
    "        ret = [0]*len(genre_dict) #number of genres we are looking at\n",
    "        for i in range(numElems):\n",
    "            genre_str = (json_genres[i]['name'])\n",
    "            if genre_str in genre_map.keys():\n",
    "                ret[genre_dict[genre_map[genre_str]]] = 1\n",
    "        return ret\n",
    "    except Exception as excep:\n",
    "        print('Exception' + str(excep))\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dictionary for genre to its index in label vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incldue ALL genres of data set in genre dict as is\n",
    "genre_dict = {'War': 0,\n",
    " 'Family': 1,\n",
    " 'Science Fiction': 2,\n",
    " 'Thriller': 3,\n",
    " 'Horror': 4,\n",
    " 'Romance': 5,\n",
    " 'Drama': 6,\n",
    " 'Foreign': 7,\n",
    " 'Documentary': 8,\n",
    " 'Fantasy': 9,\n",
    " 'Western': 10,\n",
    " 'History': 11,\n",
    " 'Comedy': 12,\n",
    " 'Action': 13,\n",
    " 'Adventure': 14,\n",
    " 'Animation': 15,\n",
    " 'Crime': 16,\n",
    " 'Music': 17,\n",
    " 'TV Movie': 18,\n",
    " 'Mystery': 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for mapping to coarse grained labels (in this situation we don't do that so labels map to self)\n",
    "#maps to self here as we don't do anything special with labels in this file\n",
    "genre_map = {'War': 'War',\n",
    " 'Family': 'Family',\n",
    " 'Science Fiction': 'Science Fiction',\n",
    " 'Thriller': 'Thriller',\n",
    " 'Horror': 'Horror',\n",
    " 'Romance': 'Romance',\n",
    " 'Drama': 'Drama',\n",
    " 'Foreign': 'Foreign',\n",
    " 'Documentary': 'Documentary',\n",
    " 'Fantasy': 'Fantasy',\n",
    " 'Western': 'Western',\n",
    " 'History': 'History',\n",
    " 'Comedy': 'Comedy',\n",
    " 'Action': 'Action',\n",
    " 'Adventure': 'Adventure',\n",
    " 'Animation': 'Animation',\n",
    " 'Crime': 'Crime',\n",
    " 'Music': 'Music',\n",
    " 'TV Movie': 'TV Movie',\n",
    " 'Mystery': 'Mystery'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGenresVects():\n",
    "    y = all_data['genres']\n",
    "    ret = y.apply(parse_genres_json)\n",
    "    all_data['genres_vect'] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGenresVects() #get label vectors for genres indexed by indexes in genre_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put to lower case, remove punctation, remove stopwords\n",
    "def cleanText(text):\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    text = ' '.join(no_stopword_text)\n",
    "    text = re.sub(r'[^a-z A-Z0-9]', \"\", text) #maybe shouldn't remove punction between words here?\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "all_data['cleanOverview'] = all_data['overview'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data[all_data.genres_vect.map(sum) > 0] #drop rows that now have no labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural net data only needs a few cols\n",
    "nn_data = all_data[['cleanOverview', 'genres_vect', 'overview']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(nn_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract actual features and labels from train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gettrian and test features for classification. Just need text and lables for this\n",
    "x = train['cleanOverview'].values.tolist()\n",
    "y = train['genres_vect']\n",
    "x_test = test['cleanOverview'].values.tolist()\n",
    "y_test = test['genres_vect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert labels from array of lists to numpy array\n",
    "\n",
    "y_train = y.tolist()\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_test = y_test.tolist()\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get initial word embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = [word_tokenize(ov) for ov in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_len = 32\n",
    "w2v = Word2Vec(tok, min_count = 2, size=word_vec_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_kept = 100000 #using 100000 most popular words, use throughout\n",
    "\n",
    "tokenizer = Tokenizer(num_words_kept)\n",
    "tokenizer.fit_on_texts(x)\n",
    "sequences = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "max_seq_len = 150 #larger than averaage but not too large\n",
    "\n",
    "#get actual train features to feed into neural nets for training\n",
    "x_train_seq = pad_sequences(sequences, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "#get actual test features to feed into neural nets for testing\n",
    "x_test_seq = pad_sequences(test_sequences, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get word embeddings matrix for start input to neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Citation: This technique to get word embeddings comes, with some minor changes, mostly from: \n",
    "#https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-11-cnn-word2vec-41f5e28eda74\n",
    "\n",
    "embeddings_index = {}\n",
    "for w in w2v.wv.vocab.keys():\n",
    "    embeddings_index[w] = w2v.wv[w]\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((num_words_kept, word_vec_len))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words_kept:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define evlaution metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_label_metrics(real_labels_matrix, predictions_labels_matrix):\n",
    "    for genre in genre_dict.keys():\n",
    "        index = genre_dict[genre]\n",
    "        real_labels_vect = real_labels_matrix[:, index]\n",
    "        prediction_vect = predictions_labels_matrix[:,index]\n",
    "        print(\"Accuruacy for \" + genre + \": \" + str(accuracy_score(real_labels_vect, prediction_vect)))\n",
    "        print(\"Precision for \" + genre + \": \" + str(precision_score(real_labels_vect, prediction_vect)))\n",
    "        print(\"Recall for \" + genre + \": \" + str(recall_score(real_labels_vect, prediction_vect)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of intersection of predicted and actual labels divided by size of their union for each datapoint tested on\n",
    "#sum those and then divide by number of datapoints\n",
    "#vectorized for speed\n",
    "def multi_label_accuracy(real_labels_matrix, predictions_labels_matrix):\n",
    "    #binary so set intersection is and operator\n",
    "    intersection = real_labels_matrix & predictions_labels_matrix\n",
    "    #set union for binary is same as or operator\n",
    "    union = real_labels_matrix | predictions_labels_matrix\n",
    "    #sum(array.T) gets number of 1s in row\n",
    "    row_wise_accuracy = sum(intersection.T) / sum(union.T)\n",
    "    return sum(row_wise_accuracy) / real_labels_matrix.shape[0]\n",
    "\n",
    "#size of intersection of predicted and actual labels divided by size of predicted set for each datapoint tested on\n",
    "#sum those and divide by number of datapoints\n",
    "#if no predicted labels, don't count that row towards the precision as that would be undefined\n",
    "def multi_label_precision(real_labels_matrix, predictions_labels_matrix):\n",
    "    #binary so set intersection is and operator\n",
    "    intersection = real_labels_matrix & predictions_labels_matrix\n",
    "    precision_sum = 0\n",
    "    num_rows = 0\n",
    "    for row in range(intersection.shape[0]):\n",
    "        if sum(predictions_labels_matrix[row]) > 0: #if there is at least one prediction for this row\n",
    "            num_rows += 1\n",
    "            precision_sum += sum(intersection[row]) / sum(predictions_labels_matrix[row])\n",
    "    if num_rows == 0:\n",
    "        return 0#no labels predicted at all will give us 0 precision as precision makes no sense here\n",
    "    return precision_sum / num_rows\n",
    "\n",
    "#size of intersection of predicted and actual labels divided by size of real label set for each datapoint tested on\n",
    "#sum those and divide by number of datapoints\n",
    "#all datapoints should have at least 1 real label in this data set\n",
    "#vectorized for speed\n",
    "def multi_label_recall(real_labels_matrix, predictions_labels_matrix):\n",
    "    #binary so set intersection is and operator\n",
    "    intersection = real_labels_matrix & predictions_labels_matrix\n",
    "    #set union for binary is same as or operator\n",
    "    #sum(array.T) gets number of 1s in row\n",
    "    row_wise_recall = sum(intersection.T) / sum(real_labels_matrix.T)\n",
    "    return sum(row_wise_recall) / real_labels_matrix.shape[0]\n",
    "\n",
    "#lower is better. Percent incorrectly chosen labels counting assignment and non-assignment equally\n",
    "def hamming_loss(real_labels_matrix, predictions_labels_matrix):\n",
    "    return (np.logical_xor(real_labels_matrix, predictions_labels_matrix)).sum()/(real_labels_matrix.shape[0] * real_labels_matrix.shape[1])\n",
    "\n",
    "\n",
    "#K is what we imported keras backend as\n",
    "\n",
    "#metric for keras for early stopping\n",
    "#takes in raw labels from kerass (not yet converted to 0 and 1s)\n",
    "#NOT the same as accuracy, this is total labels correctly identified divided by union of total labels\n",
    "#this weights rows with more labels higher, where accruacy does not, but this is still a good metric for early stopping\n",
    "def raw_multi_label_accuracy(y_true, y_pred):\n",
    "    positives = K.greater_equal(y_pred, 0.5)\n",
    "    positives = K.cast(positives, K.floatx())\n",
    "    new_y_pred = positives #+ ((1-positives)*y_pred)\n",
    "    intersection = y_true * new_y_pred\n",
    "    union = 1 -((1-y_true)*(1-new_y_pred))\n",
    "    accuracy = K.sum(intersection) / K.sum(union)\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(actual_labels, predictions):\n",
    "    print('Getting evaluation metrics for each label:')\n",
    "    get_per_label_metrics(actual_labels, predictions)\n",
    "    print('Getting evaluations for multilabel problem')\n",
    "    print('Multilabel accuracy: ' + str(multi_label_accuracy(actual_labels, predictions)))\n",
    "    print('Multilabel precision: ' + str(multi_label_precision(actual_labels, predictions)))\n",
    "    print('Multilabel recall: ' + str(multi_label_recall(actual_labels, predictions)))\n",
    "    print(\"Percent of correctly decided label decisions: \" + str(100* (1-hamming_loss(actual_labels, predictions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for early stopping only after certain number of epochs. wait until delay epochs until early stopping\n",
    "#not same as patience. Want to not even start looking until delay is reached\n",
    "class DelayedEarlyStopping(EarlyStopping):\n",
    "    def __init__(self, monitor, min_delta=0, patience=0, verbose=0, mode='auto', delay = 100):\n",
    "        super(DelayedEarlyStopping, self).__init__(monitor=monitor, min_delta=min_delta, patience=patience,verbose=verbose, mode=mode)\n",
    "        self.delay = delay\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch > self.delay:\n",
    "            super().on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_output_to_predictions(res):\n",
    "    label_predictions = []\n",
    "    for i in range(res.shape[0]):\n",
    "        pred = [0]*len(genre_dict)\n",
    "        for j in range(res.shape[1]):\n",
    "            if res[i][j] >= .5:\n",
    "                pred[j] = 1\n",
    "        label_predictions.append(pred)\n",
    "    return np.array(label_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/matt/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/matt/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/matt/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/matt/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 2149 samples, validate on 239 samples\n",
      "Epoch 1/1000\n",
      " - 3s - loss: 0.5787 - raw_multi_label_accuracy: 0.1347 - val_loss: 0.3989 - val_raw_multi_label_accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      " - 2s - loss: 0.4067 - raw_multi_label_accuracy: 0.1152 - val_loss: 0.3722 - val_raw_multi_label_accuracy: 0.1697\n",
      "Epoch 3/1000\n",
      " - 2s - loss: 0.3790 - raw_multi_label_accuracy: 0.1014 - val_loss: 0.3598 - val_raw_multi_label_accuracy: 0.1697\n",
      "Epoch 4/1000\n",
      " - 2s - loss: 0.3645 - raw_multi_label_accuracy: 0.1092 - val_loss: 0.3510 - val_raw_multi_label_accuracy: 0.0000e+00\n",
      "Epoch 5/1000\n",
      " - 2s - loss: 0.3545 - raw_multi_label_accuracy: 0.1081 - val_loss: 0.3447 - val_raw_multi_label_accuracy: 0.1700\n",
      "Epoch 6/1000\n",
      " - 2s - loss: 0.3488 - raw_multi_label_accuracy: 0.0938 - val_loss: 0.3402 - val_raw_multi_label_accuracy: 0.1697\n",
      "Epoch 7/1000\n",
      " - 2s - loss: 0.3412 - raw_multi_label_accuracy: 0.1000 - val_loss: 0.3368 - val_raw_multi_label_accuracy: 0.1686\n",
      "Epoch 8/1000\n",
      " - 2s - loss: 0.3356 - raw_multi_label_accuracy: 0.1177 - val_loss: 0.3347 - val_raw_multi_label_accuracy: 0.0256\n",
      "Epoch 9/1000\n",
      " - 2s - loss: 0.3290 - raw_multi_label_accuracy: 0.1220 - val_loss: 0.3322 - val_raw_multi_label_accuracy: 0.1588\n",
      "Epoch 10/1000\n",
      " - 2s - loss: 0.3198 - raw_multi_label_accuracy: 0.1291 - val_loss: 0.3318 - val_raw_multi_label_accuracy: 0.1655\n",
      "Epoch 11/1000\n",
      " - 2s - loss: 0.3048 - raw_multi_label_accuracy: 0.1819 - val_loss: 0.3329 - val_raw_multi_label_accuracy: 0.1519\n",
      "Epoch 12/1000\n",
      " - 2s - loss: 0.2906 - raw_multi_label_accuracy: 0.2191 - val_loss: 0.3379 - val_raw_multi_label_accuracy: 0.1554\n",
      "Epoch 13/1000\n",
      " - 2s - loss: 0.2765 - raw_multi_label_accuracy: 0.2572 - val_loss: 0.3412 - val_raw_multi_label_accuracy: 0.1632\n",
      "Epoch 14/1000\n",
      " - 2s - loss: 0.2608 - raw_multi_label_accuracy: 0.2979 - val_loss: 0.3467 - val_raw_multi_label_accuracy: 0.1742\n",
      "Epoch 15/1000\n",
      " - 2s - loss: 0.2451 - raw_multi_label_accuracy: 0.3539 - val_loss: 0.3564 - val_raw_multi_label_accuracy: 0.1768\n",
      "Epoch 16/1000\n",
      " - 2s - loss: 0.2323 - raw_multi_label_accuracy: 0.3958 - val_loss: 0.3614 - val_raw_multi_label_accuracy: 0.1852\n",
      "Epoch 17/1000\n",
      " - 2s - loss: 0.2230 - raw_multi_label_accuracy: 0.4277 - val_loss: 0.3712 - val_raw_multi_label_accuracy: 0.1836\n",
      "Epoch 18/1000\n",
      " - 2s - loss: 0.2144 - raw_multi_label_accuracy: 0.4499 - val_loss: 0.3831 - val_raw_multi_label_accuracy: 0.1817\n",
      "Epoch 19/1000\n",
      " - 1s - loss: 0.2074 - raw_multi_label_accuracy: 0.4686 - val_loss: 0.3855 - val_raw_multi_label_accuracy: 0.1854\n",
      "Epoch 20/1000\n",
      " - 1s - loss: 0.2008 - raw_multi_label_accuracy: 0.4894 - val_loss: 0.3882 - val_raw_multi_label_accuracy: 0.1891\n",
      "Epoch 21/1000\n",
      " - 1s - loss: 0.1942 - raw_multi_label_accuracy: 0.4983 - val_loss: 0.3927 - val_raw_multi_label_accuracy: 0.1832\n",
      "Epoch 22/1000\n",
      " - 1s - loss: 0.1882 - raw_multi_label_accuracy: 0.5212 - val_loss: 0.4041 - val_raw_multi_label_accuracy: 0.1919\n",
      "Epoch 23/1000\n",
      " - 1s - loss: 0.1807 - raw_multi_label_accuracy: 0.5349 - val_loss: 0.4073 - val_raw_multi_label_accuracy: 0.1918\n",
      "Epoch 24/1000\n",
      " - 1s - loss: 0.1749 - raw_multi_label_accuracy: 0.5508 - val_loss: 0.4095 - val_raw_multi_label_accuracy: 0.1902\n",
      "Epoch 25/1000\n",
      " - 2s - loss: 0.1685 - raw_multi_label_accuracy: 0.5668 - val_loss: 0.4208 - val_raw_multi_label_accuracy: 0.1882\n",
      "Epoch 26/1000\n",
      " - 2s - loss: 0.1619 - raw_multi_label_accuracy: 0.5876 - val_loss: 0.4192 - val_raw_multi_label_accuracy: 0.1832\n",
      "Epoch 27/1000\n",
      " - 2s - loss: 0.1560 - raw_multi_label_accuracy: 0.6058 - val_loss: 0.4340 - val_raw_multi_label_accuracy: 0.1835\n",
      "Epoch 28/1000\n",
      " - 2s - loss: 0.1492 - raw_multi_label_accuracy: 0.6294 - val_loss: 0.4422 - val_raw_multi_label_accuracy: 0.1872\n",
      "Epoch 29/1000\n",
      " - 2s - loss: 0.1431 - raw_multi_label_accuracy: 0.6432 - val_loss: 0.4495 - val_raw_multi_label_accuracy: 0.1902\n",
      "Epoch 30/1000\n",
      " - 2s - loss: 0.1386 - raw_multi_label_accuracy: 0.6554 - val_loss: 0.4670 - val_raw_multi_label_accuracy: 0.1892\n",
      "Epoch 31/1000\n",
      " - 2s - loss: 0.1344 - raw_multi_label_accuracy: 0.6712 - val_loss: 0.4561 - val_raw_multi_label_accuracy: 0.1954\n",
      "Epoch 32/1000\n",
      " - 2s - loss: 0.1291 - raw_multi_label_accuracy: 0.6863 - val_loss: 0.4785 - val_raw_multi_label_accuracy: 0.1926\n",
      "Epoch 33/1000\n",
      " - 2s - loss: 0.1250 - raw_multi_label_accuracy: 0.6972 - val_loss: 0.4811 - val_raw_multi_label_accuracy: 0.1976\n",
      "Epoch 34/1000\n",
      " - 2s - loss: 0.1232 - raw_multi_label_accuracy: 0.6954 - val_loss: 0.4862 - val_raw_multi_label_accuracy: 0.1911\n",
      "Epoch 35/1000\n",
      " - 2s - loss: 0.1188 - raw_multi_label_accuracy: 0.7088 - val_loss: 0.4966 - val_raw_multi_label_accuracy: 0.1901\n",
      "Epoch 36/1000\n",
      " - 2s - loss: 0.1158 - raw_multi_label_accuracy: 0.7141 - val_loss: 0.5079 - val_raw_multi_label_accuracy: 0.1949\n",
      "Epoch 37/1000\n",
      " - 2s - loss: 0.1142 - raw_multi_label_accuracy: 0.7259 - val_loss: 0.5241 - val_raw_multi_label_accuracy: 0.2001\n",
      "Epoch 38/1000\n",
      " - 2s - loss: 0.1112 - raw_multi_label_accuracy: 0.7268 - val_loss: 0.5164 - val_raw_multi_label_accuracy: 0.1978\n",
      "Epoch 39/1000\n",
      " - 2s - loss: 0.1094 - raw_multi_label_accuracy: 0.7369 - val_loss: 0.5184 - val_raw_multi_label_accuracy: 0.2034\n",
      "Epoch 40/1000\n",
      " - 2s - loss: 0.1056 - raw_multi_label_accuracy: 0.7424 - val_loss: 0.5362 - val_raw_multi_label_accuracy: 0.2037\n",
      "Epoch 41/1000\n",
      " - 2s - loss: 0.1042 - raw_multi_label_accuracy: 0.7478 - val_loss: 0.5476 - val_raw_multi_label_accuracy: 0.2026\n",
      "Epoch 42/1000\n",
      " - 2s - loss: 0.1014 - raw_multi_label_accuracy: 0.7556 - val_loss: 0.5495 - val_raw_multi_label_accuracy: 0.1972\n",
      "Epoch 43/1000\n",
      " - 2s - loss: 0.0996 - raw_multi_label_accuracy: 0.7620 - val_loss: 0.5505 - val_raw_multi_label_accuracy: 0.2025\n",
      "Epoch 44/1000\n",
      " - 2s - loss: 0.0987 - raw_multi_label_accuracy: 0.7574 - val_loss: 0.5678 - val_raw_multi_label_accuracy: 0.1993\n",
      "Epoch 45/1000\n",
      " - 2s - loss: 0.0961 - raw_multi_label_accuracy: 0.7647 - val_loss: 0.5657 - val_raw_multi_label_accuracy: 0.1961\n",
      "Time to train with cross validation for early stopping: 80.07889080047607 seconds\n"
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "e = Embedding(num_words_kept, word_vec_len, weights=[embedding_matrix], input_length=max_seq_len, trainable=True)\n",
    "#e = Embedding(num_words_kept, word_vec_len, input_length=max_seq_len, trainable=True)\n",
    "model_cnn.add(e)\n",
    "model_cnn.add(Conv1D(filters=50, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model_cnn.add(Dropout(.5))\n",
    "model_cnn.add(Dense(len(genre_dict), activation='sigmoid'))\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=[raw_multi_label_accuracy])\n",
    "start = time.time()\n",
    "model_cnn.fit(x_train_seq, y_train, validation_split = .1, callbacks = [DelayedEarlyStopping(monitor = 'val_raw_multi_label_accuracy', patience = 5, delay=25)], epochs=1000, batch_size=100, verbose=2)\n",
    "end = time.time()\n",
    "print('Time to train with cross validation for early stopping: ' + str(end-start) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_output_to_predictions(model_cnn.predict(x_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting evaluation metrics for each label:\n",
      "Accuruacy for War: 0.964824120603015\n",
      "Precision for War: 0.0\n",
      "Recall for War: 0.0\n",
      "\n",
      "Accuruacy for Family: 0.9195979899497487\n",
      "Precision for Family: 0.3333333333333333\n",
      "Recall for Family: 0.043478260869565216\n",
      "\n",
      "Accuruacy for Science Fiction: 0.8927973199329984\n",
      "Precision for Science Fiction: 0.3333333333333333\n",
      "Recall for Science Fiction: 0.03225806451612903\n",
      "\n",
      "Accuruacy for Thriller: 0.6817420435510888\n",
      "Precision for Thriller: 0.3870967741935484\n",
      "Recall for Thriller: 0.21301775147928995\n",
      "\n",
      "Accuruacy for Horror: 0.8961474036850922\n",
      "Precision for Horror: 0.0\n",
      "Recall for Horror: 0.0\n",
      "\n",
      "Accuruacy for Romance: 0.7403685092127303\n",
      "Precision for Romance: 0.2441860465116279\n",
      "Recall for Romance: 0.1891891891891892\n",
      "\n",
      "Accuruacy for Drama: 0.5862646566164154\n",
      "Precision for Drama: 0.5833333333333334\n",
      "Recall for Drama: 0.7899686520376176\n",
      "\n",
      "Accuruacy for Foreign: 0.9882747068676717\n",
      "Precision for Foreign: 0.0\n",
      "Recall for Foreign: 0.0\n",
      "\n",
      "Accuruacy for Documentary: 0.9731993299832495\n",
      "Precision for Documentary: 0.0\n",
      "Recall for Documentary: 0.0\n",
      "\n",
      "Accuruacy for Fantasy: 0.9195979899497487\n",
      "Precision for Fantasy: 0.0\n",
      "Recall for Fantasy: 0.0\n",
      "\n",
      "Accuruacy for Western: 0.983249581239531\n",
      "Precision for Western: 0.0\n",
      "Recall for Western: 0.0\n",
      "\n",
      "Accuruacy for History: 0.9614740368509213\n",
      "Precision for History: 0.0\n",
      "Recall for History: 0.0\n",
      "\n",
      "Accuruacy for Comedy: 0.5963149078726968\n",
      "Precision for Comedy: 0.3949579831932773\n",
      "Recall for Comedy: 0.2175925925925926\n",
      "\n",
      "Accuruacy for Action: 0.7085427135678392\n",
      "Precision for Action: 0.38666666666666666\n",
      "Recall for Action: 0.18471337579617833\n",
      "\n",
      "Accuruacy for Adventure: 0.8123953098827471\n",
      "Precision for Adventure: 0.20689655172413793\n",
      "Recall for Adventure: 0.06315789473684211\n",
      "\n",
      "Accuruacy for Animation: 0.9514237855946399\n",
      "Precision for Animation: 0.0\n",
      "Recall for Animation: 0.0\n",
      "\n",
      "Accuruacy for Crime: 0.7571189279731994\n",
      "Precision for Crime: 0.25\n",
      "Recall for Crime: 0.2631578947368421\n",
      "\n",
      "Accuruacy for Music: 0.9748743718592965\n",
      "Precision for Music: 0.0\n",
      "Recall for Music: 0.0\n",
      "\n",
      "Accuruacy for TV Movie: 1.0\n",
      "Precision for TV Movie: 0.0\n",
      "Recall for TV Movie: 0.0\n",
      "\n",
      "Accuruacy for Mystery: 0.9195979899497487\n",
      "Precision for Mystery: 0.1\n",
      "Recall for Mystery: 0.025\n",
      "\n",
      "Getting evaluations for multilabel problem\n",
      "Multilabel accuracy: 0.24849046821408632\n",
      "Multilabel precision: 0.46573067632850257\n",
      "Multilabel recall: 0.31221982930525666\n",
      "Percent of correctly decided label decisions: 86.13902847571188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matt/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "get_all_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN but with multiple filter sizes so we don't just filter on group of words at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2149 samples, validate on 239 samples\n",
      "Epoch 1/1000\n",
      " - 6s - loss: 2.6106 - raw_multi_label_accuracy: 0.1168 - val_loss: 1.8744 - val_raw_multi_label_accuracy: 0.1481\n",
      "Epoch 2/1000\n",
      " - 6s - loss: 1.4934 - raw_multi_label_accuracy: 0.1052 - val_loss: 1.0871 - val_raw_multi_label_accuracy: 0.1688\n",
      "Epoch 3/1000\n",
      " - 5s - loss: 0.8868 - raw_multi_label_accuracy: 0.1168 - val_loss: 0.6795 - val_raw_multi_label_accuracy: 0.1697\n",
      "Epoch 4/1000\n",
      " - 5s - loss: 0.5843 - raw_multi_label_accuracy: 0.1017 - val_loss: 0.4814 - val_raw_multi_label_accuracy: 0.1590\n",
      "Epoch 5/1000\n",
      " - 5s - loss: 0.4405 - raw_multi_label_accuracy: 0.0906 - val_loss: 0.3920 - val_raw_multi_label_accuracy: 0.1274\n",
      "Epoch 6/1000\n",
      " - 6s - loss: 0.3765 - raw_multi_label_accuracy: 0.1054 - val_loss: 0.3528 - val_raw_multi_label_accuracy: 0.1697\n",
      "Epoch 7/1000\n",
      " - 5s - loss: 0.3468 - raw_multi_label_accuracy: 0.0976 - val_loss: 0.3330 - val_raw_multi_label_accuracy: 0.0224\n",
      "Epoch 8/1000\n",
      " - 5s - loss: 0.3332 - raw_multi_label_accuracy: 0.1024 - val_loss: 0.3259 - val_raw_multi_label_accuracy: 0.1367\n",
      "Epoch 9/1000\n",
      " - 4s - loss: 0.3260 - raw_multi_label_accuracy: 0.1118 - val_loss: 0.3230 - val_raw_multi_label_accuracy: 0.0000e+00\n",
      "Epoch 10/1000\n",
      " - 4s - loss: 0.3237 - raw_multi_label_accuracy: 0.0946 - val_loss: 0.3210 - val_raw_multi_label_accuracy: 0.0000e+00\n",
      "Epoch 11/1000\n",
      " - 4s - loss: 0.3196 - raw_multi_label_accuracy: 0.1131 - val_loss: 0.3194 - val_raw_multi_label_accuracy: 0.1697\n",
      "Epoch 12/1000\n",
      " - 4s - loss: 0.3179 - raw_multi_label_accuracy: 0.1025 - val_loss: 0.3187 - val_raw_multi_label_accuracy: 0.1695\n",
      "Epoch 13/1000\n",
      " - 4s - loss: 0.3157 - raw_multi_label_accuracy: 0.1162 - val_loss: 0.3199 - val_raw_multi_label_accuracy: 0.1695\n",
      "Epoch 14/1000\n",
      " - 4s - loss: 0.3117 - raw_multi_label_accuracy: 0.1162 - val_loss: 0.3197 - val_raw_multi_label_accuracy: 0.1695\n",
      "Epoch 15/1000\n",
      " - 4s - loss: 0.3070 - raw_multi_label_accuracy: 0.1326 - val_loss: 0.3212 - val_raw_multi_label_accuracy: 0.1636\n",
      "Epoch 16/1000\n",
      " - 5s - loss: 0.3006 - raw_multi_label_accuracy: 0.1222 - val_loss: 0.3221 - val_raw_multi_label_accuracy: 0.1611\n",
      "Epoch 17/1000\n",
      " - 6s - loss: 0.2962 - raw_multi_label_accuracy: 0.1407 - val_loss: 0.3229 - val_raw_multi_label_accuracy: 0.1449\n",
      "Epoch 18/1000\n",
      " - 6s - loss: 0.2910 - raw_multi_label_accuracy: 0.1314 - val_loss: 0.3223 - val_raw_multi_label_accuracy: 0.1404\n",
      "Epoch 19/1000\n",
      " - 7s - loss: 0.2861 - raw_multi_label_accuracy: 0.1449 - val_loss: 0.3201 - val_raw_multi_label_accuracy: 0.1084\n",
      "Epoch 20/1000\n",
      " - 8s - loss: 0.2821 - raw_multi_label_accuracy: 0.1663 - val_loss: 0.3250 - val_raw_multi_label_accuracy: 0.1060\n",
      "Epoch 21/1000\n",
      " - 8s - loss: 0.2777 - raw_multi_label_accuracy: 0.1718 - val_loss: 0.3223 - val_raw_multi_label_accuracy: 0.1470\n",
      "Epoch 22/1000\n",
      " - 8s - loss: 0.2740 - raw_multi_label_accuracy: 0.1904 - val_loss: 0.3232 - val_raw_multi_label_accuracy: 0.1409\n",
      "Epoch 23/1000\n",
      " - 8s - loss: 0.2687 - raw_multi_label_accuracy: 0.2161 - val_loss: 0.3259 - val_raw_multi_label_accuracy: 0.1050\n",
      "Epoch 24/1000\n",
      " - 5s - loss: 0.2639 - raw_multi_label_accuracy: 0.2296 - val_loss: 0.3254 - val_raw_multi_label_accuracy: 0.1125\n",
      "Epoch 25/1000\n",
      " - 8s - loss: 0.2587 - raw_multi_label_accuracy: 0.2452 - val_loss: 0.3261 - val_raw_multi_label_accuracy: 0.1442\n",
      "Epoch 26/1000\n",
      " - 8s - loss: 0.2535 - raw_multi_label_accuracy: 0.2650 - val_loss: 0.3285 - val_raw_multi_label_accuracy: 0.1249\n",
      "Epoch 27/1000\n",
      " - 9s - loss: 0.2465 - raw_multi_label_accuracy: 0.2944 - val_loss: 0.3318 - val_raw_multi_label_accuracy: 0.1423\n",
      "Epoch 28/1000\n",
      " - 8s - loss: 0.2374 - raw_multi_label_accuracy: 0.3380 - val_loss: 0.3362 - val_raw_multi_label_accuracy: 0.1591\n",
      "Epoch 29/1000\n",
      " - 5s - loss: 0.2267 - raw_multi_label_accuracy: 0.3839 - val_loss: 0.3377 - val_raw_multi_label_accuracy: 0.1697\n",
      "Epoch 30/1000\n",
      " - 4s - loss: 0.2157 - raw_multi_label_accuracy: 0.4229 - val_loss: 0.3414 - val_raw_multi_label_accuracy: 0.1713\n",
      "Epoch 31/1000\n",
      " - 4s - loss: 0.2080 - raw_multi_label_accuracy: 0.4474 - val_loss: 0.3438 - val_raw_multi_label_accuracy: 0.1745\n",
      "Epoch 32/1000\n",
      " - 4s - loss: 0.2016 - raw_multi_label_accuracy: 0.4648 - val_loss: 0.3507 - val_raw_multi_label_accuracy: 0.1878\n",
      "Epoch 33/1000\n",
      " - 5s - loss: 0.1984 - raw_multi_label_accuracy: 0.4751 - val_loss: 0.3604 - val_raw_multi_label_accuracy: 0.1686\n",
      "Epoch 34/1000\n",
      " - 4s - loss: 0.1944 - raw_multi_label_accuracy: 0.4774 - val_loss: 0.3582 - val_raw_multi_label_accuracy: 0.1906\n",
      "Epoch 35/1000\n",
      " - 5s - loss: 0.1905 - raw_multi_label_accuracy: 0.4877 - val_loss: 0.3612 - val_raw_multi_label_accuracy: 0.1884\n",
      "Epoch 36/1000\n",
      " - 4s - loss: 0.1878 - raw_multi_label_accuracy: 0.4945 - val_loss: 0.3743 - val_raw_multi_label_accuracy: 0.1835\n",
      "Epoch 37/1000\n",
      " - 4s - loss: 0.1850 - raw_multi_label_accuracy: 0.4980 - val_loss: 0.3732 - val_raw_multi_label_accuracy: 0.1814\n",
      "Epoch 38/1000\n",
      " - 4s - loss: 0.1832 - raw_multi_label_accuracy: 0.5022 - val_loss: 0.3774 - val_raw_multi_label_accuracy: 0.1879\n",
      "Epoch 39/1000\n",
      " - 4s - loss: 0.1810 - raw_multi_label_accuracy: 0.5025 - val_loss: 0.3823 - val_raw_multi_label_accuracy: 0.1662\n",
      "Time to train with cross validation for early stopping: 218.0960373878479 seconds\n"
     ]
    }
   ],
   "source": [
    "model_input = Input(shape=(max_seq_len,), dtype='int32')\n",
    "e = Embedding(num_words_kept, word_vec_len, weights=[embedding_matrix], input_length=max_seq_len, trainable=True)(model_input)\n",
    "two_word_filter = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(e)\n",
    "two_word_filter = GlobalMaxPooling1D()(two_word_filter)\n",
    "three_word_filter = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(e)\n",
    "three_word_filter = GlobalMaxPooling1D()(three_word_filter)\n",
    "four_word_filter = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(e)\n",
    "four_word_filter = GlobalMaxPooling1D()(four_word_filter)\n",
    "merged = concatenate([two_word_filter, three_word_filter, four_word_filter], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "merged = Dense(len(genre_dict))(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[model_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[raw_multi_label_accuracy])\n",
    "start = time.time()\n",
    "model.fit(x_train_seq, y_train, validation_split = .1, callbacks = [DelayedEarlyStopping(monitor = 'val_raw_multi_label_accuracy', patience = 5, delay=25)], epochs=1000, batch_size=100, verbose=2)\n",
    "end = time.time()\n",
    "print('Time to train with cross validation for early stopping: ' + str(end-start) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_output_to_predictions(model.predict(x_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting evaluation metrics for each label:\n",
      "Accuruacy for War: 0.964824120603015\n",
      "Precision for War: 0.0\n",
      "Recall for War: 0.0\n",
      "\n",
      "Accuruacy for Family: 0.9229480737018425\n",
      "Precision for Family: 0.0\n",
      "Recall for Family: 0.0\n",
      "\n",
      "Accuruacy for Science Fiction: 0.8944723618090452\n",
      "Precision for Science Fiction: 0.0\n",
      "Recall for Science Fiction: 0.0\n",
      "\n",
      "Accuruacy for Thriller: 0.6884422110552764\n",
      "Precision for Thriller: 0.4105263157894737\n",
      "Recall for Thriller: 0.23076923076923078\n",
      "\n",
      "Accuruacy for Horror: 0.9061976549413735\n",
      "Precision for Horror: 0.0\n",
      "Recall for Horror: 0.0\n",
      "\n",
      "Accuruacy for Romance: 0.8123953098827471\n",
      "Precision for Romance: 0.4782608695652174\n",
      "Recall for Romance: 0.0990990990990991\n",
      "\n",
      "Accuruacy for Drama: 0.5678391959798995\n",
      "Precision for Drama: 0.5927051671732523\n",
      "Recall for Drama: 0.6112852664576802\n",
      "\n",
      "Accuruacy for Foreign: 0.9882747068676717\n",
      "Precision for Foreign: 0.0\n",
      "Recall for Foreign: 0.0\n",
      "\n",
      "Accuruacy for Documentary: 0.9731993299832495\n",
      "Precision for Documentary: 0.0\n",
      "Recall for Documentary: 0.0\n",
      "\n",
      "Accuruacy for Fantasy: 0.9212730318257957\n",
      "Precision for Fantasy: 0.0\n",
      "Recall for Fantasy: 0.0\n",
      "\n",
      "Accuruacy for Western: 0.983249581239531\n",
      "Precision for Western: 0.0\n",
      "Recall for Western: 0.0\n",
      "\n",
      "Accuruacy for History: 0.9614740368509213\n",
      "Precision for History: 0.0\n",
      "Recall for History: 0.0\n",
      "\n",
      "Accuruacy for Comedy: 0.628140703517588\n",
      "Precision for Comedy: 0.4634146341463415\n",
      "Recall for Comedy: 0.17592592592592593\n",
      "\n",
      "Accuruacy for Action: 0.7303182579564489\n",
      "Precision for Action: 0.4583333333333333\n",
      "Recall for Action: 0.14012738853503184\n",
      "\n",
      "Accuruacy for Adventure: 0.8408710217755444\n",
      "Precision for Adventure: 0.5\n",
      "Recall for Adventure: 0.021052631578947368\n",
      "\n",
      "Accuruacy for Animation: 0.9547738693467337\n",
      "Precision for Animation: 0.0\n",
      "Recall for Animation: 0.0\n",
      "\n",
      "Accuruacy for Crime: 0.8341708542713567\n",
      "Precision for Crime: 0.4166666666666667\n",
      "Recall for Crime: 0.10526315789473684\n",
      "\n",
      "Accuruacy for Music: 0.9748743718592965\n",
      "Precision for Music: 0.0\n",
      "Recall for Music: 0.0\n",
      "\n",
      "Accuruacy for TV Movie: 1.0\n",
      "Precision for TV Movie: 0.0\n",
      "Recall for TV Movie: 0.0\n",
      "\n",
      "Accuruacy for Mystery: 0.932998324958124\n",
      "Precision for Mystery: 0.0\n",
      "Recall for Mystery: 0.0\n",
      "\n",
      "Getting evaluations for multilabel problem\n",
      "Multilabel accuracy: 0.2096514317619845\n",
      "Multilabel precision: 0.5378787878787878\n",
      "Multilabel recall: 0.23460556752014036\n",
      "Percent of correctly decided label decisions: 87.4036850921273\n"
     ]
    }
   ],
   "source": [
    "get_all_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2149 samples, validate on 239 samples\n",
      "Epoch 1/1000\n",
      " - 3s - loss: 0.4020 - raw_multi_label_accuracy: 0.1224 - val_loss: 0.3433 - val_raw_multi_label_accuracy: 0.0373\n",
      "Epoch 2/1000\n",
      " - 2s - loss: 0.3228 - raw_multi_label_accuracy: 0.1162 - val_loss: 0.3297 - val_raw_multi_label_accuracy: 0.1159\n",
      "Epoch 3/1000\n",
      " - 2s - loss: 0.3062 - raw_multi_label_accuracy: 0.1260 - val_loss: 0.3284 - val_raw_multi_label_accuracy: 0.0363\n",
      "Epoch 4/1000\n",
      " - 2s - loss: 0.2952 - raw_multi_label_accuracy: 0.1330 - val_loss: 0.3218 - val_raw_multi_label_accuracy: 0.1126\n",
      "Epoch 5/1000\n",
      " - 2s - loss: 0.2852 - raw_multi_label_accuracy: 0.1635 - val_loss: 0.3218 - val_raw_multi_label_accuracy: 0.1571\n",
      "Epoch 6/1000\n",
      " - 2s - loss: 0.2697 - raw_multi_label_accuracy: 0.2038 - val_loss: 0.3242 - val_raw_multi_label_accuracy: 0.1445\n",
      "Epoch 7/1000\n",
      " - 2s - loss: 0.2491 - raw_multi_label_accuracy: 0.2592 - val_loss: 0.3191 - val_raw_multi_label_accuracy: 0.1517\n",
      "Epoch 8/1000\n",
      " - 2s - loss: 0.2235 - raw_multi_label_accuracy: 0.3469 - val_loss: 0.3156 - val_raw_multi_label_accuracy: 0.1293\n",
      "Epoch 9/1000\n",
      " - 2s - loss: 0.1970 - raw_multi_label_accuracy: 0.4423 - val_loss: 0.3155 - val_raw_multi_label_accuracy: 0.1622\n",
      "Epoch 10/1000\n",
      " - 2s - loss: 0.1713 - raw_multi_label_accuracy: 0.5384 - val_loss: 0.3198 - val_raw_multi_label_accuracy: 0.1706\n",
      "Epoch 11/1000\n",
      " - 2s - loss: 0.1500 - raw_multi_label_accuracy: 0.6090 - val_loss: 0.3263 - val_raw_multi_label_accuracy: 0.1459\n",
      "Epoch 12/1000\n",
      " - 2s - loss: 0.1290 - raw_multi_label_accuracy: 0.6791 - val_loss: 0.3395 - val_raw_multi_label_accuracy: 0.1484\n",
      "Epoch 13/1000\n",
      " - 2s - loss: 0.1110 - raw_multi_label_accuracy: 0.7263 - val_loss: 0.3436 - val_raw_multi_label_accuracy: 0.1698\n",
      "Epoch 14/1000\n",
      " - 2s - loss: 0.0941 - raw_multi_label_accuracy: 0.7835 - val_loss: 0.3563 - val_raw_multi_label_accuracy: 0.1654\n",
      "Epoch 15/1000\n",
      " - 2s - loss: 0.0795 - raw_multi_label_accuracy: 0.8325 - val_loss: 0.3743 - val_raw_multi_label_accuracy: 0.1694\n",
      "Epoch 16/1000\n",
      " - 3s - loss: 0.0673 - raw_multi_label_accuracy: 0.8688 - val_loss: 0.3946 - val_raw_multi_label_accuracy: 0.1669\n",
      "Epoch 17/1000\n",
      " - 2s - loss: 0.0567 - raw_multi_label_accuracy: 0.8936 - val_loss: 0.3970 - val_raw_multi_label_accuracy: 0.1726\n",
      "Epoch 18/1000\n",
      " - 3s - loss: 0.0478 - raw_multi_label_accuracy: 0.9141 - val_loss: 0.4118 - val_raw_multi_label_accuracy: 0.1832\n",
      "Epoch 19/1000\n",
      " - 2s - loss: 0.0398 - raw_multi_label_accuracy: 0.9403 - val_loss: 0.4307 - val_raw_multi_label_accuracy: 0.1734\n",
      "Epoch 20/1000\n",
      " - 2s - loss: 0.0333 - raw_multi_label_accuracy: 0.9558 - val_loss: 0.4536 - val_raw_multi_label_accuracy: 0.1688\n",
      "Epoch 21/1000\n",
      " - 2s - loss: 0.0282 - raw_multi_label_accuracy: 0.9650 - val_loss: 0.4484 - val_raw_multi_label_accuracy: 0.1789\n",
      "Epoch 22/1000\n",
      " - 2s - loss: 0.0239 - raw_multi_label_accuracy: 0.9747 - val_loss: 0.4767 - val_raw_multi_label_accuracy: 0.1719\n",
      "Epoch 23/1000\n",
      " - 2s - loss: 0.0201 - raw_multi_label_accuracy: 0.9792 - val_loss: 0.4970 - val_raw_multi_label_accuracy: 0.1753\n",
      "Epoch 24/1000\n",
      " - 3s - loss: 0.0171 - raw_multi_label_accuracy: 0.9846 - val_loss: 0.5055 - val_raw_multi_label_accuracy: 0.1666\n",
      "Epoch 25/1000\n",
      " - 3s - loss: 0.0146 - raw_multi_label_accuracy: 0.9876 - val_loss: 0.5217 - val_raw_multi_label_accuracy: 0.1721\n",
      "Epoch 26/1000\n",
      " - 2s - loss: 0.0125 - raw_multi_label_accuracy: 0.9915 - val_loss: 0.5325 - val_raw_multi_label_accuracy: 0.1755\n",
      "Epoch 27/1000\n",
      " - 2s - loss: 0.0108 - raw_multi_label_accuracy: 0.9925 - val_loss: 0.5437 - val_raw_multi_label_accuracy: 0.1756\n",
      "Epoch 28/1000\n",
      " - 2s - loss: 0.0094 - raw_multi_label_accuracy: 0.9950 - val_loss: 0.5602 - val_raw_multi_label_accuracy: 0.1770\n",
      "Epoch 29/1000\n",
      " - 2s - loss: 0.0082 - raw_multi_label_accuracy: 0.9962 - val_loss: 0.5711 - val_raw_multi_label_accuracy: 0.1802\n",
      "Epoch 30/1000\n",
      " - 2s - loss: 0.0072 - raw_multi_label_accuracy: 0.9963 - val_loss: 0.5842 - val_raw_multi_label_accuracy: 0.1773\n",
      "Epoch 31/1000\n",
      " - 2s - loss: 0.0064 - raw_multi_label_accuracy: 0.9978 - val_loss: 0.5923 - val_raw_multi_label_accuracy: 0.1790\n",
      "Epoch 32/1000\n",
      " - 2s - loss: 0.0056 - raw_multi_label_accuracy: 0.9981 - val_loss: 0.6009 - val_raw_multi_label_accuracy: 0.1729\n",
      "Epoch 33/1000\n",
      " - 2s - loss: 0.0050 - raw_multi_label_accuracy: 0.9981 - val_loss: 0.6135 - val_raw_multi_label_accuracy: 0.1784\n",
      "Epoch 34/1000\n",
      " - 2s - loss: 0.0045 - raw_multi_label_accuracy: 0.9985 - val_loss: 0.6211 - val_raw_multi_label_accuracy: 0.1745\n",
      "Time to train with cross validation for early stopping: 71.74203610420227 seconds\n"
     ]
    }
   ],
   "source": [
    "normal_nn = Sequential()\n",
    "e = Embedding(num_words_kept, word_vec_len, weights=[embedding_matrix], input_length=max_seq_len, trainable=True)\n",
    "normal_nn.add(e)\n",
    "normal_nn.add(Flatten())\n",
    "normal_nn.add(Dense(256, activation='relu'))\n",
    "normal_nn.add(Dense(len(genre_dict), activation='sigmoid'))\n",
    "normal_nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=[raw_multi_label_accuracy])\n",
    "start = time.time()\n",
    "normal_nn.fit(x_train_seq, y_train, validation_split = .1, callbacks = [DelayedEarlyStopping(monitor = 'val_raw_multi_label_accuracy', patience = 5, delay=25)], epochs=1000, batch_size=100, verbose=2)\n",
    "end = time.time()\n",
    "print('Time to train with cross validation for early stopping: ' + str(end-start) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_output_to_predictions(normal_nn.predict(x_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting evaluation metrics for each label:\n",
      "Accuruacy for War: 0.964824120603015\n",
      "Precision for War: 0.0\n",
      "Recall for War: 0.0\n",
      "\n",
      "Accuruacy for Family: 0.9229480737018425\n",
      "Precision for Family: 0.0\n",
      "Recall for Family: 0.0\n",
      "\n",
      "Accuruacy for Science Fiction: 0.9028475711892797\n",
      "Precision for Science Fiction: 1.0\n",
      "Recall for Science Fiction: 0.06451612903225806\n",
      "\n",
      "Accuruacy for Thriller: 0.7085427135678392\n",
      "Precision for Thriller: 0.4358974358974359\n",
      "Recall for Thriller: 0.10059171597633136\n",
      "\n",
      "Accuruacy for Horror: 0.9045226130653267\n",
      "Precision for Horror: 0.3333333333333333\n",
      "Recall for Horror: 0.017857142857142856\n",
      "\n",
      "Accuruacy for Romance: 0.8257956448911222\n",
      "Precision for Romance: 0.5897435897435898\n",
      "Recall for Romance: 0.2072072072072072\n",
      "\n",
      "Accuruacy for Drama: 0.5845896147403685\n",
      "Precision for Drama: 0.6011396011396012\n",
      "Recall for Drama: 0.6614420062695925\n",
      "\n",
      "Accuruacy for Foreign: 0.9882747068676717\n",
      "Precision for Foreign: 0.0\n",
      "Recall for Foreign: 0.0\n",
      "\n",
      "Accuruacy for Documentary: 0.9731993299832495\n",
      "Precision for Documentary: 0.0\n",
      "Recall for Documentary: 0.0\n",
      "\n",
      "Accuruacy for Fantasy: 0.9179229480737019\n",
      "Precision for Fantasy: 0.2\n",
      "Recall for Fantasy: 0.021739130434782608\n",
      "\n",
      "Accuruacy for Western: 0.983249581239531\n",
      "Precision for Western: 0.0\n",
      "Recall for Western: 0.0\n",
      "\n",
      "Accuruacy for History: 0.9614740368509213\n",
      "Precision for History: 0.0\n",
      "Recall for History: 0.0\n",
      "\n",
      "Accuruacy for Comedy: 0.6348408710217756\n",
      "Precision for Comedy: 0.48936170212765956\n",
      "Recall for Comedy: 0.21296296296296297\n",
      "\n",
      "Accuruacy for Action: 0.7286432160804021\n",
      "Precision for Action: 0.4418604651162791\n",
      "Recall for Action: 0.12101910828025478\n",
      "\n",
      "Accuruacy for Adventure: 0.8341708542713567\n",
      "Precision for Adventure: 0.3\n",
      "Recall for Adventure: 0.031578947368421054\n",
      "\n",
      "Accuruacy for Animation: 0.9547738693467337\n",
      "Precision for Animation: 0.0\n",
      "Recall for Animation: 0.0\n",
      "\n",
      "Accuruacy for Crime: 0.8458961474036851\n",
      "Precision for Crime: 0.6666666666666666\n",
      "Recall for Crime: 0.06315789473684211\n",
      "\n",
      "Accuruacy for Music: 0.9731993299832495\n",
      "Precision for Music: 0.0\n",
      "Recall for Music: 0.0\n",
      "\n",
      "Accuruacy for TV Movie: 1.0\n",
      "Precision for TV Movie: 0.0\n",
      "Recall for TV Movie: 0.0\n",
      "\n",
      "Accuruacy for Mystery: 0.9279731993299832\n",
      "Precision for Mystery: 0.0\n",
      "Recall for Mystery: 0.0\n",
      "\n",
      "Getting evaluations for multilabel problem\n",
      "Multilabel accuracy: 0.23133125947196306\n",
      "Multilabel precision: 0.5463784183296377\n",
      "Multilabel recall: 0.25239291696578114\n",
      "Percent of correctly decided label decisions: 87.68844221105527\n"
     ]
    }
   ],
   "source": [
    "get_all_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2149 samples, validate on 239 samples\n",
      "Epoch 1/1000\n",
      " - 25s - loss: 0.5003 - raw_multi_label_accuracy: 0.1572 - val_loss: 0.3284 - val_raw_multi_label_accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      " - 10s - loss: 0.3181 - raw_multi_label_accuracy: 0.0866 - val_loss: 0.3164 - val_raw_multi_label_accuracy: 0.1695\n",
      "Epoch 3/1000\n",
      " - 9s - loss: 0.3128 - raw_multi_label_accuracy: 0.0921 - val_loss: 0.3155 - val_raw_multi_label_accuracy: 0.0160\n",
      "Epoch 4/1000\n",
      " - 9s - loss: 0.3123 - raw_multi_label_accuracy: 0.1275 - val_loss: 0.3142 - val_raw_multi_label_accuracy: 0.0553\n",
      "Epoch 5/1000\n",
      " - 10s - loss: 0.3120 - raw_multi_label_accuracy: 0.0890 - val_loss: 0.3144 - val_raw_multi_label_accuracy: 0.0816\n",
      "Epoch 6/1000\n",
      " - 12s - loss: 0.3117 - raw_multi_label_accuracy: 0.0857 - val_loss: 0.3146 - val_raw_multi_label_accuracy: 0.0127\n",
      "Epoch 7/1000\n",
      " - 12s - loss: 0.3118 - raw_multi_label_accuracy: 0.1395 - val_loss: 0.3143 - val_raw_multi_label_accuracy: 0.0322\n",
      "Epoch 8/1000\n",
      " - 9s - loss: 0.3113 - raw_multi_label_accuracy: 0.0809 - val_loss: 0.3155 - val_raw_multi_label_accuracy: 0.1695\n",
      "Epoch 9/1000\n",
      " - 9s - loss: 0.3109 - raw_multi_label_accuracy: 0.1025 - val_loss: 0.3144 - val_raw_multi_label_accuracy: 0.0988\n",
      "Epoch 10/1000\n",
      " - 11s - loss: 0.3109 - raw_multi_label_accuracy: 0.1071 - val_loss: 0.3160 - val_raw_multi_label_accuracy: 0.0080\n",
      "Epoch 11/1000\n",
      " - 11s - loss: 0.3096 - raw_multi_label_accuracy: 0.0872 - val_loss: 0.3151 - val_raw_multi_label_accuracy: 0.0542\n",
      "Epoch 12/1000\n",
      " - 9s - loss: 0.3086 - raw_multi_label_accuracy: 0.1246 - val_loss: 0.3144 - val_raw_multi_label_accuracy: 0.1488\n",
      "Epoch 13/1000\n",
      " - 9s - loss: 0.3054 - raw_multi_label_accuracy: 0.1050 - val_loss: 0.3132 - val_raw_multi_label_accuracy: 0.0584\n",
      "Epoch 14/1000\n",
      " - 9s - loss: 0.2983 - raw_multi_label_accuracy: 0.1185 - val_loss: 0.3122 - val_raw_multi_label_accuracy: 0.1673\n",
      "Epoch 15/1000\n",
      " - 9s - loss: 0.2857 - raw_multi_label_accuracy: 0.1420 - val_loss: 0.3100 - val_raw_multi_label_accuracy: 0.1155\n",
      "Epoch 16/1000\n",
      " - 11s - loss: 0.2751 - raw_multi_label_accuracy: 0.1937 - val_loss: 0.3074 - val_raw_multi_label_accuracy: 0.1594\n",
      "Epoch 17/1000\n",
      " - 9s - loss: 0.2668 - raw_multi_label_accuracy: 0.2211 - val_loss: 0.3097 - val_raw_multi_label_accuracy: 0.1618\n",
      "Epoch 18/1000\n",
      " - 9s - loss: 0.2604 - raw_multi_label_accuracy: 0.2599 - val_loss: 0.3110 - val_raw_multi_label_accuracy: 0.1577\n",
      "Epoch 19/1000\n",
      " - 9s - loss: 0.2527 - raw_multi_label_accuracy: 0.2952 - val_loss: 0.3138 - val_raw_multi_label_accuracy: 0.2107\n",
      "Epoch 20/1000\n",
      " - 9s - loss: 0.2442 - raw_multi_label_accuracy: 0.3247 - val_loss: 0.3176 - val_raw_multi_label_accuracy: 0.2086\n",
      "Epoch 21/1000\n",
      " - 9s - loss: 0.2311 - raw_multi_label_accuracy: 0.3806 - val_loss: 0.3246 - val_raw_multi_label_accuracy: 0.2093\n",
      "Epoch 22/1000\n",
      " - 9s - loss: 0.2181 - raw_multi_label_accuracy: 0.4073 - val_loss: 0.3224 - val_raw_multi_label_accuracy: 0.2016\n",
      "Epoch 23/1000\n",
      " - 9s - loss: 0.2055 - raw_multi_label_accuracy: 0.4458 - val_loss: 0.3304 - val_raw_multi_label_accuracy: 0.2072\n",
      "Epoch 24/1000\n",
      " - 9s - loss: 0.1938 - raw_multi_label_accuracy: 0.4698 - val_loss: 0.3434 - val_raw_multi_label_accuracy: 0.2097\n",
      "Epoch 25/1000\n",
      " - 9s - loss: 0.1847 - raw_multi_label_accuracy: 0.5016 - val_loss: 0.3457 - val_raw_multi_label_accuracy: 0.1902\n",
      "Epoch 26/1000\n",
      " - 8s - loss: 0.1770 - raw_multi_label_accuracy: 0.5059 - val_loss: 0.3526 - val_raw_multi_label_accuracy: 0.2041\n",
      "Epoch 27/1000\n",
      " - 8s - loss: 0.1685 - raw_multi_label_accuracy: 0.5328 - val_loss: 0.3756 - val_raw_multi_label_accuracy: 0.2154\n",
      "Epoch 28/1000\n",
      " - 9s - loss: 0.1612 - raw_multi_label_accuracy: 0.5536 - val_loss: 0.3718 - val_raw_multi_label_accuracy: 0.2065\n",
      "Epoch 29/1000\n",
      " - 14s - loss: 0.1521 - raw_multi_label_accuracy: 0.5658 - val_loss: 0.3859 - val_raw_multi_label_accuracy: 0.2217\n",
      "Epoch 30/1000\n",
      " - 16s - loss: 0.1459 - raw_multi_label_accuracy: 0.5881 - val_loss: 0.3913 - val_raw_multi_label_accuracy: 0.2287\n",
      "Epoch 31/1000\n",
      " - 12s - loss: 0.1378 - raw_multi_label_accuracy: 0.6169 - val_loss: 0.3992 - val_raw_multi_label_accuracy: 0.2221\n",
      "Epoch 32/1000\n",
      " - 15s - loss: 0.1296 - raw_multi_label_accuracy: 0.6352 - val_loss: 0.3995 - val_raw_multi_label_accuracy: 0.2298\n",
      "Epoch 33/1000\n",
      " - 13s - loss: 0.1235 - raw_multi_label_accuracy: 0.6489 - val_loss: 0.4132 - val_raw_multi_label_accuracy: 0.2050\n",
      "Epoch 34/1000\n",
      " - 9s - loss: 0.1158 - raw_multi_label_accuracy: 0.6621 - val_loss: 0.4296 - val_raw_multi_label_accuracy: 0.2271\n",
      "Epoch 35/1000\n",
      " - 9s - loss: 0.1092 - raw_multi_label_accuracy: 0.6920 - val_loss: 0.4385 - val_raw_multi_label_accuracy: 0.2319\n",
      "Epoch 36/1000\n",
      " - 9s - loss: 0.1027 - raw_multi_label_accuracy: 0.7133 - val_loss: 0.4527 - val_raw_multi_label_accuracy: 0.2286\n",
      "Epoch 37/1000\n",
      " - 9s - loss: 0.0968 - raw_multi_label_accuracy: 0.7295 - val_loss: 0.4754 - val_raw_multi_label_accuracy: 0.2416\n",
      "Epoch 38/1000\n",
      " - 9s - loss: 0.0926 - raw_multi_label_accuracy: 0.7359 - val_loss: 0.4786 - val_raw_multi_label_accuracy: 0.2351\n",
      "Epoch 39/1000\n",
      " - 9s - loss: 0.0866 - raw_multi_label_accuracy: 0.7551 - val_loss: 0.4922 - val_raw_multi_label_accuracy: 0.2295\n",
      "Epoch 40/1000\n",
      " - 12s - loss: 0.0817 - raw_multi_label_accuracy: 0.7706 - val_loss: 0.5108 - val_raw_multi_label_accuracy: 0.2333\n",
      "Epoch 41/1000\n",
      " - 11s - loss: 0.0768 - raw_multi_label_accuracy: 0.7859 - val_loss: 0.5247 - val_raw_multi_label_accuracy: 0.2393\n",
      "Epoch 42/1000\n",
      " - 9s - loss: 0.0726 - raw_multi_label_accuracy: 0.7975 - val_loss: 0.5379 - val_raw_multi_label_accuracy: 0.2356\n",
      "Time to train with cross validation for early stopping: 435.1609230041504 seconds\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "e = Embedding(num_words_kept, word_vec_len, weights=[embedding_matrix], input_length=max_seq_len, trainable=True)\n",
    "lstm_model.add(e)\n",
    "lstm_model.add(LSTM(100, dropout=0.25, recurrent_dropout=0.25))\n",
    "lstm_model.add(Dense(256, activation='relu'))\n",
    "lstm_model.add(Dense(len(genre_dict), activation='sigmoid'))\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[raw_multi_label_accuracy])\n",
    "start = time.time()\n",
    "lstm_model.fit(x_train_seq, y_train, validation_split = .1, callbacks = [DelayedEarlyStopping(monitor = 'val_raw_multi_label_accuracy', patience = 5, delay=25)], epochs=1000, batch_size=100, verbose=2)\n",
    "end = time.time()\n",
    "print('Time to train with cross validation for early stopping: ' + str(end-start) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_output_to_predictions(lstm_model.predict(x_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting evaluation metrics for each label:\n",
      "Accuruacy for War: 0.964824120603015\n",
      "Precision for War: 0.0\n",
      "Recall for War: 0.0\n",
      "\n",
      "Accuruacy for Family: 0.916247906197655\n",
      "Precision for Family: 0.25\n",
      "Recall for Family: 0.043478260869565216\n",
      "\n",
      "Accuruacy for Science Fiction: 0.8760469011725294\n",
      "Precision for Science Fiction: 0.3235294117647059\n",
      "Recall for Science Fiction: 0.1774193548387097\n",
      "\n",
      "Accuruacy for Thriller: 0.695142378559464\n",
      "Precision for Thriller: 0.4496124031007752\n",
      "Recall for Thriller: 0.3431952662721893\n",
      "\n",
      "Accuruacy for Horror: 0.8777219430485762\n",
      "Precision for Horror: 0.2702702702702703\n",
      "Recall for Horror: 0.17857142857142858\n",
      "\n",
      "Accuruacy for Romance: 0.7889447236180904\n",
      "Precision for Romance: 0.4117647058823529\n",
      "Recall for Romance: 0.3153153153153153\n",
      "\n",
      "Accuruacy for Drama: 0.5745393634840871\n",
      "Precision for Drama: 0.599388379204893\n",
      "Recall for Drama: 0.6144200626959248\n",
      "\n",
      "Accuruacy for Foreign: 0.9882747068676717\n",
      "Precision for Foreign: 0.0\n",
      "Recall for Foreign: 0.0\n",
      "\n",
      "Accuruacy for Documentary: 0.9698492462311558\n",
      "Precision for Documentary: 0.0\n",
      "Recall for Documentary: 0.0\n",
      "\n",
      "Accuruacy for Fantasy: 0.9078726968174204\n",
      "Precision for Fantasy: 0.2857142857142857\n",
      "Recall for Fantasy: 0.13043478260869565\n",
      "\n",
      "Accuruacy for Western: 0.983249581239531\n",
      "Precision for Western: 0.0\n",
      "Recall for Western: 0.0\n",
      "\n",
      "Accuruacy for History: 0.9597989949748744\n",
      "Precision for History: 0.3333333333333333\n",
      "Recall for History: 0.043478260869565216\n",
      "\n",
      "Accuruacy for Comedy: 0.5862646566164154\n",
      "Precision for Comedy: 0.4311111111111111\n",
      "Recall for Comedy: 0.44907407407407407\n",
      "\n",
      "Accuruacy for Action: 0.7085427135678392\n",
      "Precision for Action: 0.4444444444444444\n",
      "Recall for Action: 0.43312101910828027\n",
      "\n",
      "Accuruacy for Adventure: 0.8174204355108877\n",
      "Precision for Adventure: 0.36538461538461536\n",
      "Recall for Adventure: 0.2\n",
      "\n",
      "Accuruacy for Animation: 0.9547738693467337\n",
      "Precision for Animation: 0.0\n",
      "Recall for Animation: 0.0\n",
      "\n",
      "Accuruacy for Crime: 0.8190954773869347\n",
      "Precision for Crime: 0.4392523364485981\n",
      "Recall for Crime: 0.49473684210526314\n",
      "\n",
      "Accuruacy for Music: 0.9731993299832495\n",
      "Precision for Music: 0.0\n",
      "Recall for Music: 0.0\n",
      "\n",
      "Accuruacy for TV Movie: 1.0\n",
      "Precision for TV Movie: 0.0\n",
      "Recall for TV Movie: 0.0\n",
      "\n",
      "Accuruacy for Mystery: 0.8994974874371859\n",
      "Precision for Mystery: 0.08333333333333333\n",
      "Recall for Mystery: 0.05\n",
      "\n",
      "Getting evaluations for multilabel problem\n",
      "Multilabel accuracy: 0.28884103054957316\n",
      "Multilabel precision: 0.46934369602763387\n",
      "Multilabel recall: 0.3846255084948553\n",
      "Percent of correctly decided label decisions: 86.30653266331659\n"
     ]
    }
   ],
   "source": [
    "get_all_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2149 samples, validate on 239 samples\n",
      "Epoch 1/1000\n",
      " - 3s - loss: 0.5408 - raw_multi_label_accuracy: 0.0644 - val_loss: 0.3440 - val_raw_multi_label_accuracy: 0.1695\n",
      "Epoch 2/1000\n",
      " - 2s - loss: 0.3323 - raw_multi_label_accuracy: 0.1113 - val_loss: 0.3253 - val_raw_multi_label_accuracy: 0.1682\n",
      "Epoch 3/1000\n",
      " - 1s - loss: 0.3197 - raw_multi_label_accuracy: 0.1197 - val_loss: 0.3206 - val_raw_multi_label_accuracy: 0.0845\n",
      "Epoch 4/1000\n",
      " - 2s - loss: 0.3126 - raw_multi_label_accuracy: 0.1305 - val_loss: 0.3158 - val_raw_multi_label_accuracy: 0.1429\n",
      "Epoch 5/1000\n",
      " - 2s - loss: 0.3054 - raw_multi_label_accuracy: 0.1205 - val_loss: 0.3126 - val_raw_multi_label_accuracy: 0.1624\n",
      "Epoch 6/1000\n",
      " - 2s - loss: 0.2985 - raw_multi_label_accuracy: 0.1397 - val_loss: 0.3110 - val_raw_multi_label_accuracy: 0.0888\n",
      "Epoch 7/1000\n",
      " - 2s - loss: 0.2909 - raw_multi_label_accuracy: 0.1392 - val_loss: 0.3097 - val_raw_multi_label_accuracy: 0.1399\n",
      "Epoch 8/1000\n"
     ]
    }
   ],
   "source": [
    "rnn = Sequential()\n",
    "e = Embedding(num_words_kept, word_vec_len, weights=[embedding_matrix], input_length=max_seq_len, trainable=True)\n",
    "rnn.add(e)\n",
    "rnn.add(SimpleRNN(32, activation = 'relu'))\n",
    "rnn.add(Dense(256, activation='relu'))\n",
    "rnn.add(Dense(len(genre_dict), activation='sigmoid'))\n",
    "rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=[raw_multi_label_accuracy])\n",
    "start = time.time()\n",
    "rnn.fit(x_train_seq, y_train, validation_split = .1, callbacks = [DelayedEarlyStopping(monitor = 'val_raw_multi_label_accuracy', patience = 5, delay=25)], epochs=1000, batch_size=100, verbose=2)\n",
    "end = time.time()\n",
    "print('Time to train with cross validation for early stopping: ' + str(end-start) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_output_to_predictions(rnn.predict(x_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_all_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bidirectional-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bi_lstm = Sequential()\n",
    "e = Embedding(num_words_kept, word_vec_len, weights=[embedding_matrix], input_length=max_seq_len, trainable=True)\n",
    "bi_lstm.add(e)\n",
    "bi_lstm.add(Bidirectional(LSTM(100, dropout=0.25, recurrent_dropout=0.25)))\n",
    "bi_lstm.add(Dense(256, activation='relu'))\n",
    "bi_lstm.add(Dense(len(genre_dict), activation='sigmoid'))\n",
    "bi_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=[raw_multi_label_accuracy])\n",
    "start = time.time()\n",
    "bi_lstm.fit(x_train_seq, y_train, validation_split = .1, callbacks = [DelayedEarlyStopping(monitor = 'val_raw_multi_label_accuracy', patience = 5, delay=25)], epochs=1000, batch_size=100, verbose=2)\n",
    "end = time.time()\n",
    "print('Time to train with cross validation for early stopping: ' + str(end-start) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_output_to_predictions(bi_lstm.predict(x_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
