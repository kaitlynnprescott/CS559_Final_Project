{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Label k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-322098c6dbf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "all_data = pd.read_csv('train.csv')\n",
    "all_data = all_data.dropna(subset=['overview', 'genres']) #drop cols without overview or genre (data we use or labels)\n",
    "genre_set = {'Comedy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all irrelevant columns\n",
    "all_data = all_data.drop(columns=['id', 'belongs_to_collection', 'budget','homepage','imdb_id','original_language','original_title','popularity','poster_path','production_companies','production_countries','release_date','runtime','spoken_languages','status','tagline','title','Keywords','cast','crew','revenue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "<ul>\n",
    "    <li>Get all possible genres.</li>\n",
    "    <li>Vectorize Genres:</li>\n",
    "    <ul>\n",
    "        <li>Save genres as a vector of 0s and 1s.</li>\n",
    "        <li>Save genres as list of strings.</li>\n",
    "    </ul>\n",
    "    <li>Separete genres into binary columns.</li>\n",
    "    <li>Drop columns no longer needed.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing functions\n",
    "def text_to_list(x):\n",
    "    if pd.isna(x):\n",
    "        return ''\n",
    "    else:\n",
    "        return ast.literal_eval(x)\n",
    "\n",
    "    \n",
    "def parse_json(x):\n",
    "    try:\n",
    "        return json.loads(x.replace(\"'\", '\"'))[0]['name']\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "def parse_all_genres_json(x):\n",
    "    try:\n",
    "        json_genres = json.loads(x.replace(\"'\", '\"'))\n",
    "        numElems = len(json_genres)\n",
    "        for i in range(numElems):\n",
    "            genre_set.add(json_genres[i]['name'])\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "def parse_genres_json(x):\n",
    "    try:\n",
    "        json_genres = json.loads(x.replace(\"'\", '\"'))\n",
    "        numElems = len(json_genres)\n",
    "        ret = [0]*len(genre_dict) #20 0s\n",
    "        for i in range(numElems):\n",
    "            ret[genre_dict[(json_genres[i]['name'])]] = 1\n",
    "        return ret\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "\n",
    "def get_labels_as_strs(x):\n",
    "    try:\n",
    "        json_genres = json.loads(x.replace(\"'\", '\"'))\n",
    "        numElems = len(json_genres)\n",
    "        ret = []#20 0s\n",
    "        for i in range(numElems):\n",
    "            ret.append(json_genres[i]['name'])\n",
    "        return ret\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of genres\n",
    "def getAllGenres():\n",
    "    full_data = pd.read_csv('train.csv')\n",
    "    y = full_data['genres']\n",
    "    y.apply(parse_all_genres_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAllGenres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get set to dictionary for indexing of target vectors\n",
    "genre_dict = {}\n",
    "index = 0\n",
    "for genre in genre_set:\n",
    "    genre_dict[genre] = index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize genres\n",
    "def getGenresVects():\n",
    "    y = all_data['genres']\n",
    "    ret = y.apply(parse_genres_json)\n",
    "    all_data['genres_vect'] = ret\n",
    "    label_strs = y.apply(get_labels_as_strs)\n",
    "    all_data['genres_labels'] = label_strs\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGenresVects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify data for 6 categories instead of 20\n",
    "action_adventure = []\n",
    "comedy = []\n",
    "drama = []\n",
    "horror_triller = []\n",
    "romance = []\n",
    "scifi_fantasy = []\n",
    "\n",
    "# loop through all rows\n",
    "for i in range(all_data.shape[0]):\n",
    "    # get the labels\n",
    "    labels = all_data.iloc[i,3]\n",
    "    # append a 1 if the genre is in the labels, and a 0 otherwise\n",
    "    # reducing number of categories and combining very similar categories\n",
    "    # that are usually together\n",
    "    if 'Action' in labels:\n",
    "        action_adventure.append(1)\n",
    "    elif 'Adventure' in labels:\n",
    "        action_adventure.append(1)\n",
    "    else:\n",
    "        action_adventure.append(0)\n",
    "\n",
    "    if 'Comedy' in labels:\n",
    "        comedy.append(1)\n",
    "    else:\n",
    "        comedy.append(0)\n",
    "\n",
    "    if 'Drama' in labels:\n",
    "        drama.append(1)\n",
    "    else:\n",
    "        drama.append(0)\n",
    "\n",
    "    if 'Horror'in labels:\n",
    "        horror_triller.append(1)\n",
    "    elif 'Thriller'in labels:\n",
    "        horror_triller.append(1)\n",
    "    else:\n",
    "        horror_triller.append(0)\n",
    "\n",
    "    if 'Romance'in labels:\n",
    "        romance.append(1)\n",
    "    else:\n",
    "        romance.append(0)\n",
    "\n",
    "    if 'Science Fiction'in labels:\n",
    "        scifi_fantasy.append(1)\n",
    "    elif 'Fantasy' in labels:\n",
    "        scifi_fantasy.append(1)\n",
    "    else:\n",
    "        scifi_fantasy.append(0)\n",
    "        \n",
    "# set new column in data frame to be lists just generated above\n",
    "all_data[\"Action-Adventure\"] = action_adventure\n",
    "all_data[\"Comedy\"] = comedy\n",
    "all_data[\"Drama\"] = drama\n",
    "all_data[\"Horror-Thriller\"] = horror_triller\n",
    "all_data[\"Romance\"] = romance\n",
    "all_data[\"SciFi-Fantasy\"] = scifi_fantasy\n",
    "\n",
    "categories = ['Action-Adventure', 'Comedy', 'Drama', 'Horror-Thriller', 'Romance', 'SciFi-Fantasy']\n",
    "\n",
    "# recheck all rows for genres, and remove rows with none\n",
    "sum_genres = []\n",
    "for i in range(all_data.shape[0]):\n",
    "    row = all_data.iloc[i]\n",
    "    sum_row = 0\n",
    "    for category in categories:\n",
    "        if row[category] == 1:\n",
    "            sum_row = 1\n",
    "    if sum_row == 0:\n",
    "        sum_genres.append(None)\n",
    "    else:\n",
    "        sum_genres.append(sum_row)\n",
    "        \n",
    "all_data[\"numGenres\"] = sum_genres\n",
    "all_data = all_data.dropna(subset=['numGenres']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns no longer needed\n",
    "data = all_data.drop(columns=['genres','genres_vect','genres_labels','numGenres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Bar Plots for data visualization\n",
    "<ul>\n",
    "    <li>Number of Movies in Each Genre</li>\n",
    "    <li>Number of Movies in Reduced Genres</li>\n",
    "    <li>Number of Movies with Multiple Labels</li>\n",
    "    <li>Number of Movies with Multiple Labels in Reduced Genres</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot movies in each genre (6)\n",
    "sns.set(font_scale=2)\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "\n",
    "y = data.iloc[:,1:].sum().values\n",
    "\n",
    "ax= sns.barplot(categories[1:], y, palette=\"plasma\")\n",
    "plt.title(\"Movies in each Genre\", fontsize=24)\n",
    "plt.ylabel('Number of Movies', fontsize=18)\n",
    "plt.xlabel('Genre', fontsize=18)\n",
    "plt.xticks(fontsize=10)\n",
    "\n",
    "rects = ax.patches\n",
    "labels = data.iloc[:,1:].sum().values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot how many movies with multiple labels (in 6)\n",
    "row_sums = data.iloc[:,1:].sum(axis=1)\n",
    "multilabel_counts = row_sums.value_counts()\n",
    "multilabel_counts = multilabel_counts.iloc[1:]\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "ax = sns.barplot(multilabel_counts.index, multilabel_counts.values, palette=\"plasma\")\n",
    "\n",
    "plt.title(\"Movies with multiple labels\")\n",
    "plt.ylabel('Number of movies', fontsize=18)\n",
    "plt.xlabel('Number of labels', fontsize=18)\n",
    "\n",
    "rects = ax.patches\n",
    "labels = multilabel_counts.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height+5, label, ha='center', va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Word Clouds for Genre Visualization\n",
    "Word Clouds for:\n",
    "<ul>\n",
    "    <li>Action-Adventure Movies</li>\n",
    "    <li>Drama Movies</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot word cloud for action-adventure category\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "plt.figure(figsize=(40,25))\n",
    "\n",
    "subset = data[data['Action-Adventure'] == True]\n",
    "text = subset.overview.values\n",
    "cloud_genre = WordCloud(stopwords=STOPWORDS, \n",
    "                        background_color='black',\n",
    "                        collocations=False,\n",
    "                        width=2500,\n",
    "                        height=1800).generate(\" \".join(text))\n",
    "plt.axis('off')\n",
    "plt.title(\"Action-Adventure\")\n",
    "plt.imshow(cloud_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot word cloud for Drama category\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "plt.figure(figsize=(40,25))\n",
    "\n",
    "subset = data[data['Drama'] == True]\n",
    "text = subset.overview.values\n",
    "cloud_genre = WordCloud(stopwords=STOPWORDS, \n",
    "                        background_color='black',\n",
    "                        collocations=False,\n",
    "                        width=2500,\n",
    "                        height=1800).generate(\" \".join(text))\n",
    "plt.axis('off')\n",
    "plt.title(\"Drama\")\n",
    "plt.imshow(cloud_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Overview Category\n",
    "Processing:\n",
    "<ul>\n",
    "    <li>Remove punctuation</li>\n",
    "    <li>Set to lowercase</li>\n",
    "    <li>Remove StopWords -> words that are common in the english language</li>\n",
    "    <ul><li>Ex. the, a, etc.</li></ul>\n",
    "    <li>Stem words -> reduce to root words</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data\n",
    "# ignore warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "#put to lower case, remove punctation\n",
    "def cleanText(text):\n",
    "    text = re.sub(r'[^a-z A-Z 0-9]', \"\", text) #maybe shouldn't remove punction between words here?\n",
    "    text = text.lower()\n",
    "    return text\n",
    "df['overview'] = df['overview'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "\n",
    "# remove common words in the english langauge (like the, a, etc.)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "df['overview'] = df['overview'].apply(removeStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# reduce words to the root i.e. running -> run\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "df['overview'] = df['overview'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, random_state=42, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorize strings to be interpreted by knn\n",
    "X_train = train['overview']\n",
    "y_train = train[1:]\n",
    "\n",
    "X_test = test['overview']\n",
    "y_test = test[1:]\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "# vectorie all data\n",
    "vectorizer.fit(X_train)\n",
    "vectorizer.fit(X_test)\n",
    "vectorizer.fit(y_train)\n",
    "vectorizer.fit(y_test)\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "y_train = train.drop(labels=['overview'], axis=1)\n",
    "\n",
    "X_test = vectorizer.transform(X_test)\n",
    "y_test = test.drop(labels=['overview'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Multi-Label k-Nearest Neighbors\n",
    "<ul>\n",
    "    <li>Use k=25 neighbors</li>\n",
    "    <li>Transform data into list matrices</li>\n",
    "    <li>Train and Predict</li>\n",
    "    <li>Display Metrics</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_max(arr):\n",
    "    mx = arr[0]\n",
    "    ind_mx = 0\n",
    "    for i in range(1,len(arr)):\n",
    "        if arr[i] >= mx:\n",
    "            mx = arr[i]\n",
    "            ind_mx = i\n",
    "    k_ind = ind_mx\n",
    "    return (k_ind, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "k_range = np.array(range(1,31))\n",
    "k_scores = []\n",
    "\n",
    "X_train = lil_matrix(X_train).toarray()\n",
    "y_train = lil_matrix(y_train).toarray()\n",
    "X_test = lil_matrix(X_test).toarray()\n",
    "    \n",
    "for k in k_range:\n",
    "    clf = MLkNN(k=k)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "    \n",
    "mx = last_max(k_scores)\n",
    "print('Max of list', mx[1])\n",
    "best_k = mx[0]+1\n",
    "print(best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-validated accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Multi-Label k-Nearest Neighbors\n",
    "classifier = MLkNN(k=best_k)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = {:.2f}%\".format(accuracy_score(y_test, predictions) * 100.0))\n",
    "print(\"Precision = {:.2f}%\".format(precision_score(y_test, predictions, average=\"samples\") * 100.0))\n",
    "print(\"Recall = {:.2f}%\".format(recall_score(y_test, predictions, average=\"samples\")* 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
