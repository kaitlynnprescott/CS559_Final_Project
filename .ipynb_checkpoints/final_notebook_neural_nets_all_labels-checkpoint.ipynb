{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#for reading in data properly\n",
    "import ast\n",
    "import json\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, LSTM, SimpleRNN, Dense, Dropout, Flatten\n",
    "from keras.layers import Input, concatenate, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('train.csv')\n",
    "all_data = all_data.dropna(subset=['overview', 'genres']) #drop cols without overview or genre (data we use or labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse each row to get label vectors from json\n",
    "def parse_genres_json(x):\n",
    "    try:\n",
    "        json_genres = json.loads(x.replace(\"'\", '\"'))\n",
    "        numElems = len(json_genres)\n",
    "        ret = [0]*len(genre_dict) #number of genres we are looking at\n",
    "        for i in range(numElems):\n",
    "            genre_str = (json_genres[i]['name'])\n",
    "            if genre_str in genre_map.keys():\n",
    "                ret[genre_dict[genre_map[genre_str]]] = 1\n",
    "        return ret\n",
    "    except Exception as excep:\n",
    "        print('Exception' + str(excep))\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dictionary for genre to its index in label vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incldue ALL genres of data set in genre dict as is\n",
    "genre_dict = {'War': 0,\n",
    " 'Family': 1,\n",
    " 'Science Fiction': 2,\n",
    " 'Thriller': 3,\n",
    " 'Horror': 4,\n",
    " 'Romance': 5,\n",
    " 'Drama': 6,\n",
    " 'Foreign': 7,\n",
    " 'Documentary': 8,\n",
    " 'Fantasy': 9,\n",
    " 'Western': 10,\n",
    " 'History': 11,\n",
    " 'Comedy': 12,\n",
    " 'Action': 13,\n",
    " 'Adventure': 14,\n",
    " 'Animation': 15,\n",
    " 'Crime': 16,\n",
    " 'Music': 17,\n",
    " 'TV Movie': 18,\n",
    " 'Mystery': 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for mapping to coarse grained labels (in this situation we don't do that so labels map to self)\n",
    "#maps to self here as we don't do anything special with labels in this file\n",
    "genre_map = {'War': 'War',\n",
    " 'Family': 'Family',\n",
    " 'Science Fiction': 'Science Fiction',\n",
    " 'Thriller': 'Thriller',\n",
    " 'Horror': 'Horror',\n",
    " 'Romance': 'Romance',\n",
    " 'Drama': 'Drama',\n",
    " 'Foreign': 'Foreign',\n",
    " 'Documentary': 'Documentary',\n",
    " 'Fantasy': 'Fantasy',\n",
    " 'Western': 'Western',\n",
    " 'History': 'History',\n",
    " 'Comedy': 'Comedy',\n",
    " 'Action': 'Action',\n",
    " 'Adventure': 'Adventure',\n",
    " 'Animation': 'Animation',\n",
    " 'Crime': 'Crime',\n",
    " 'Music': 'Music',\n",
    " 'TV Movie': 'TV Movie',\n",
    " 'Mystery': 'Mystery'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGenresVects():\n",
    "    y = all_data['genres']\n",
    "    ret = y.apply(parse_genres_json)\n",
    "    all_data['genres_vect'] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGenresVects() #get label vectors for genres indexed by indexes in genre_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put to lower case, remove punctation, remove stopwords\n",
    "def cleanText(text):\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    text = ' '.join(no_stopword_text)\n",
    "    text = re.sub(r'[^a-z A-Z0-9]', \"\", text) #maybe shouldn't remove punction between words here?\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "all_data['cleanOverview'] = all_data['overview'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data[all_data.genres_vect.map(sum) > 0] #drop rows that now have no labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural net data only needs a few cols\n",
    "nn_data = all_data[['cleanOverview', 'genres_vect', 'overview']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(nn_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract actual features and labels from train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gettrian and test features for classification. Just need text and lables for this\n",
    "x = train['cleanOverview'].values.tolist()\n",
    "y = train['genres_vect']\n",
    "x_test = test['cleanOverview'].values.tolist()\n",
    "y_test = test['genres_vect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert labels from array of lists to numpy array\n",
    "\n",
    "y_train = y.tolist()\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_test = y_test.tolist()\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get initial word embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = [word_tokenize(ov) for ov in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_len = 32\n",
    "w2v = Word2Vec(tok, min_count = 2, size=word_vec_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_kept = 100000 #using 100000 most popular words, use throughout\n",
    "\n",
    "tokenizer = Tokenizer(num_words_kept)\n",
    "tokenizer.fit_on_texts(x)\n",
    "sequences = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "max_seq_len = 150 #larger than averaage but not too large\n",
    "\n",
    "#get actual train features to feed into neural nets for training\n",
    "x_train_seq = pad_sequences(sequences, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "#get actual test features to feed into neural nets for testing\n",
    "x_test_seq = pad_sequences(test_sequences, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get word embeddings matrix for start input to neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Citation: This technique to get word embeddings comes, with some minor changes, mostly from: \n",
    "#https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-11-cnn-word2vec-41f5e28eda74\n",
    "\n",
    "embeddings_index = {}\n",
    "for w in w2v.wv.vocab.keys():\n",
    "    embeddings_index[w] = w2v.wv[w]\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((num_words_kept, word_vec_len))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words_kept:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define evlaution metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_label_metrics(real_labels_matrix, predictions_labels_matrix):\n",
    "    for genre in genre_dict.keys():\n",
    "        index = genre_dict[genre]\n",
    "        real_labels_vect = real_labels_matrix[:, index]\n",
    "        prediction_vect = predictions_labels_matrix[:,index]\n",
    "        print(\"Accuruacy for \" + genre + \": \" + str(accuracy_score(real_labels_vect, prediction_vect)))\n",
    "        print(\"Precision for \" + genre + \": \" + str(precision_score(real_labels_vect, prediction_vect)))\n",
    "        print(\"Recall for \" + genre + \": \" + str(recall_score(real_labels_vect, prediction_vect)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of intersection of predicted and actual labels divided by size of their union for each datapoint tested on\n",
    "#sum those and then divide by number of datapoints\n",
    "#vectorized for speed\n",
    "def multi_label_accuracy(real_labels_matrix, predictions_labels_matrix):\n",
    "    #binary so set intersection is and operator\n",
    "    intersection = real_labels_matrix & predictions_labels_matrix\n",
    "    #set union for binary is same as or operator\n",
    "    union = real_labels_matrix | predictions_labels_matrix\n",
    "    #sum(array.T) gets number of 1s in row\n",
    "    row_wise_accuracy = sum(intersection.T) / sum(union.T)\n",
    "    return sum(row_wise_accuracy) / real_labels_matrix.shape[0]\n",
    "\n",
    "#size of intersection of predicted and actual labels divided by size of predicted set for each datapoint tested on\n",
    "#sum those and divide by number of datapoints\n",
    "#if no predicted labels, don't count that row towards the precision as that would be undefined\n",
    "def multi_label_precision(real_labels_matrix, predictions_labels_matrix):\n",
    "    #binary so set intersection is and operator\n",
    "    intersection = real_labels_matrix & predictions_labels_matrix\n",
    "    precision_sum = 0\n",
    "    num_rows = 0\n",
    "    for row in range(intersection.shape[0]):\n",
    "        if sum(predictions_labels_matrix[row]) > 0: #if there is at least one prediction for this row\n",
    "            num_rows += 1\n",
    "            precision_sum += sum(intersection[row]) / sum(predictions_labels_matrix[row])\n",
    "    if num_rows == 0:\n",
    "        return 0#no labels predicted at all will give us 0 precision as precision makes no sense here\n",
    "    return precision_sum / num_rows\n",
    "\n",
    "#size of intersection of predicted and actual labels divided by size of real label set for each datapoint tested on\n",
    "#sum those and divide by number of datapoints\n",
    "#all datapoints should have at least 1 real label in this data set\n",
    "#vectorized for speed\n",
    "def multi_label_recall(real_labels_matrix, predictions_labels_matrix):\n",
    "    #binary so set intersection is and operator\n",
    "    intersection = real_labels_matrix & predictions_labels_matrix\n",
    "    #set union for binary is same as or operator\n",
    "    #sum(array.T) gets number of 1s in row\n",
    "    row_wise_recall = sum(intersection.T) / sum(real_labels_matrix.T)\n",
    "    return sum(row_wise_recall) / real_labels_matrix.shape[0]\n",
    "\n",
    "#lower is better. Percent incorrectly chosen labels counting assignment and non-assignment equally\n",
    "def hamming_loss(real_labels_matrix, predictions_labels_matrix):\n",
    "    return (np.logical_xor(real_labels_matrix, predictions_labels_matrix)).sum()/(real_labels_matrix.shape[0] * real_labels_matrix.shape[1])\n",
    "\n",
    "\n",
    "#K is what we imported keras backend as\n",
    "\n",
    "#metric for keras for early stopping\n",
    "#takes in raw labels from kerass (not yet converted to 0 and 1s)\n",
    "#NOT the same as accuracy, this is total labels correctly identified divided by union of total labels\n",
    "#this weights rows with more labels higher, where accruacy does not, but this is still a good metric for early stopping\n",
    "def raw_multi_label_accuracy(y_true, y_pred):\n",
    "    positives = K.greater_equal(y_pred, 0.5)\n",
    "    positives = K.cast(positives, K.floatx())\n",
    "    new_y_pred = positives #+ ((1-positives)*y_pred)\n",
    "    intersection = y_true * new_y_pred\n",
    "    union = 1 -((1-y_true)*(1-new_y_pred))\n",
    "    accuracy = K.sum(intersection) / K.sum(union)\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(actual_labels, predictions):\n",
    "    print('Getting evaluation metrics for each label:')\n",
    "    get_per_label_metrics(actual_labels, predictions)\n",
    "    print('Getting evaluations for multilabel problem')\n",
    "    print('Multilabel accuracy: ' + str(multi_label_accuracy(actual_labels, predictions)))\n",
    "    print('Multilabel precision: ' + str(multi_label_precision(actual_labels, predictions)))\n",
    "    print('Multilabel recall: ' + str(multi_label_recall(actual_labels, predictions)))\n",
    "    print(\"Percent of correctly decided label decisions: \" + str(100* (1-hamming_loss(actual_labels, predictions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for early stopping only after certain number of epochs. wait until delay epochs until early stopping\n",
    "#not same as patience. Want to not even start looking until delay is reached\n",
    "class DelayedEarlyStopping(EarlyStopping):\n",
    "    def __init__(self, monitor, min_delta=0, patience=0, verbose=0, mode='auto', delay = 100):\n",
    "        super(DelayedEarlyStopping, self).__init__(monitor=monitor, min_delta=min_delta, patience=patience,verbose=verbose, mode=mode)\n",
    "        self.delay = delay\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch > self.delay:\n",
    "            super().on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_output_to_predictions(res):\n",
    "    label_predictions = []\n",
    "    for i in range(res.shape[0]):\n",
    "        pred = [0]*len(genre_dict)\n",
    "        for j in range(res.shape[1]):\n",
    "            if res[i][j] >= .5:\n",
    "                pred[j] = 1\n",
    "        label_predictions.append(pred)\n",
    "    return np.array(label_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_cnn = Sequential()\n",
    "e = Embedding(num_words_kept, word_vec_len, weights=[embedding_matrix], input_length=max_seq_len, trainable=True)\n",
    "#e = Embedding(num_words_kept, word_vec_len, input_length=max_seq_len, trainable=True)\n",
    "model_cnn.add(e)\n",
    "model_cnn.add(Conv1D(filters=50, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model_cnn.add(Dropout(.5))\n",
    "model_cnn.add(Dense(len(genre_dict), activation='sigmoid'))\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=[raw_multi_label_accuracy])\n",
    "start = time.time()\n",
    "model_cnn.fit(x_train_seq, y_train, validation_split = .1, callbacks = [DelayedEarlyStopping(monitor = 'val_raw_multi_label_accuracy', patience = 5, delay=25)], epochs=1000, batch_size=100, verbose=2)\n",
    "end = time.time()\n",
    "print('Time to train with cross validation for early stopping: ' + str(end-start) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_output_to_predictions(model_cnn.predict(x_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN but with multiple filter sizes so we don't just filter on group of words at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(max_seq_len,), dtype='int32')\n",
    "e = Embedding(num_words_kept, word_vec_len, weights=[embedding_matrix], input_length=max_seq_len, trainable=True)(model_input)\n",
    "two_word_filter = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(e)\n",
    "two_word_filter = GlobalMaxPooling1D()(two_word_filter)\n",
    "three_word_filter = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(e)\n",
    "three_word_filter = GlobalMaxPooling1D()(three_word_filter)\n",
    "four_word_filter = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(e)\n",
    "four_word_filter = GlobalMaxPooling1D()(four_word_filter)\n",
    "merged = concatenate([two_word_filter, three_word_filter, four_word_filter], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "merged = Dense(len(genre_dict))(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[model_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[raw_multi_label_accuracy])\n",
    "start = time.time()\n",
    "model.fit(x_train_seq, y_train, validation_split = .1, callbacks = [DelayedEarlyStopping(monitor = 'val_raw_multi_label_accuracy', patience = 5, delay=25)], epochs=1000, batch_size=100, verbose=2)\n",
    "end = time.time()\n",
    "print('Time to train with cross validation for early stopping: ' + str(end-start) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_output_to_predictions(model.predict(x_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_nn = Sequential()\n",
    "e = Embedding(num_words_kept, word_vec_len, weights=[embedding_matrix], input_length=max_seq_len, trainable=True)\n",
    "normal_nn.add(e)\n",
    "normal_nn.add(Flatten())\n",
    "normal_nn.add(Dense(256, activation='relu'))\n",
    "normal_nn.add(Dense(len(genre_dict), activation='sigmoid'))\n",
    "normal_nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=[raw_multi_label_accuracy])\n",
    "start = time.time()\n",
    "normal_nn.fit(x_train_seq, y_train, validation_split = .1, callbacks = [DelayedEarlyStopping(monitor = 'val_raw_multi_label_accuracy', patience = 5, delay=25)], epochs=1000, batch_size=100, verbose=2)\n",
    "end = time.time()\n",
    "print('Time to train with cross validation for early stopping: ' + str(end-start) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_output_to_predictions(normal_nn.predict(x_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential()\n",
    "e = Embedding(num_words_kept, word_vec_len, weights=[embedding_matrix], input_length=max_seq_len, trainable=True)\n",
    "lstm_model.add(e)\n",
    "lstm_model.add(LSTM(100, dropout=0.25, recurrent_dropout=0.25))\n",
    "lstm_model.add(Dense(256, activation='relu'))\n",
    "lstm_model.add(Dense(len(genre_dict), activation='sigmoid'))\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[raw_multi_label_accuracy])\n",
    "start = time.time()\n",
    "lstm_model.fit(x_train_seq, y_train, validation_split = .1, callbacks = [DelayedEarlyStopping(monitor = 'val_raw_multi_label_accuracy', patience = 5, delay=25)], epochs=1000, batch_size=100, verbose=2)\n",
    "end = time.time()\n",
    "print('Time to train with cross validation for early stopping: ' + str(end-start) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_output_to_predictions(lstm_model.predict(x_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Sequential()\n",
    "e = Embedding(num_words_kept, word_vec_len, weights=[embedding_matrix], input_length=max_seq_len, trainable=True)\n",
    "rnn.add(e)\n",
    "rnn.add(SimpleRNN(32, activation = 'relu'))\n",
    "rnn.add(Dense(256, activation='relu'))\n",
    "rnn.add(Dense(len(genre_dict), activation='sigmoid'))\n",
    "rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=[raw_multi_label_accuracy])\n",
    "start = time.time()\n",
    "rnn.fit(x_train_seq, y_train, validation_split = .1, callbacks = [DelayedEarlyStopping(monitor = 'val_raw_multi_label_accuracy', patience = 5, delay=25)], epochs=1000, batch_size=100, verbose=2)\n",
    "end = time.time()\n",
    "print('Time to train with cross validation for early stopping: ' + str(end-start) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_output_to_predictions(rnn.predict(x_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_metrics(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
