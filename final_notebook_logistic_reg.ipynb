{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#for reading in data properly\n",
    "import ast\n",
    "import json\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import utils\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('train.csv')\n",
    "all_data = all_data.dropna(subset=['overview', 'genres']) #drop cols without overview or genre (data we use or labels)\n",
    "genre_set = {'Comedy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get set of all genres in dataset    \n",
    "def parse_all_genres_json(x):\n",
    "    try:\n",
    "        json_genres = json.loads(x.replace(\"'\", '\"'))\n",
    "        numElems = len(json_genres)\n",
    "        for i in range(numElems):\n",
    "            genre_set.add(json_genres[i]['name'])\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "#parse genres of specific example in dataset to get their label vector\n",
    "def parse_genres_json(x):\n",
    "    try:\n",
    "        json_genres = json.loads(x.replace(\"'\", '\"'))\n",
    "        numElems = len(json_genres)\n",
    "        ret = [0]*len(genre_dict) #20 0s\n",
    "        for i in range(numElems):\n",
    "            ret[genre_dict[(json_genres[i]['name'])]] = 1\n",
    "        return ret\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "#get list of labels this row has in string format\n",
    "def get_labels_as_strs(x):\n",
    "    try:\n",
    "        json_genres = json.loads(x.replace(\"'\", '\"'))\n",
    "        numElems = len(json_genres)\n",
    "        ret = []\n",
    "        for i in range(numElems):\n",
    "            ret.append(json_genres[i]['name'])\n",
    "        return ret\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllGenres():\n",
    "    y = all_data['genres']\n",
    "    y.apply(parse_all_genres_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAllGenres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get set to dictionary for indexing of label vectors\n",
    "genre_dict = {}\n",
    "index = 0\n",
    "for genre in genre_set:\n",
    "    genre_dict[genre] = index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGenresVects():\n",
    "    y = all_data['genres']\n",
    "    ret = y.apply(parse_genres_json)\n",
    "    all_data['genres_vect'] = ret\n",
    "    label_strs = y.apply(get_labels_as_strs) #not currently used but could be useful\n",
    "    all_data['genres_labels'] = label_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGenresVects() #get labels in vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put to lower case, remove punctation, remove stopwords\n",
    "def cleanText(text):\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    text = ' '.join(no_stopword_text)\n",
    "    text = re.sub(r'[^a-z A-Z0-9]', \"\", text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "#clean up the overview field and put it in cleanOverview\n",
    "all_data['cleanOverview'] = all_data['overview'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression data\n",
    "lr_data = all_data[['cleanOverview', 'genres_labels', 'genres_vect']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split and getting text features and labels vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(lr_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.cleanOverview\n",
    "X_test = test.cleanOverview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert labels from array of lists to numpy array\n",
    "\n",
    "train_targets_arr = train['genres_vect'].tolist()\n",
    "train_targets_arr = np.array(train_targets_arr)\n",
    "\n",
    "test_targets_arr = test['genres_vect'].tolist()\n",
    "test_targets_arr = np.array(test_targets_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define class that will do multilabel logistic regression by wrapping Pipelines of tfidf and OneVsRest Logistic Regression Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelLogisitcRegression():\n",
    "    def __init__(self, genre_dict):\n",
    "        self.genre_dict = genre_dict\n",
    "        self.pipelines = {}\n",
    "        for category in self.genre_dict.keys():\n",
    "            self.pipelines[category]=Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear', multi_class='ovr'), n_jobs=1)),\n",
    "            ])\n",
    "        \n",
    "    def fit(self, X_train, train_targets_arr):\n",
    "        start = time.time()\n",
    "        for category in self.genre_dict.keys():\n",
    "            print('Processing {}'.format(category))\n",
    "            self.pipelines[category].fit(X_train, train_targets_arr[:,genre_dict[category]])\n",
    "        end = time.time()\n",
    "        print('Time to train ' + str(end-start) + ' seconds')\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        Ret = np.zeros((X_test.shape[0],len(self.genre_dict.keys())), dtype='int')\n",
    "        for category in self.genre_dict.keys():\n",
    "            try:\n",
    "                prediction = self.pipelines[category].predict(X_test)\n",
    "            except: #exception we get is it was trained with data taht was only 0 label during cross validation\n",
    "                prediction = np.zeros(X_test.shape[0], dtype=int)\n",
    "            Ret[:,self.genre_dict[category]] = prediction\n",
    "        return Ret\n",
    "    \n",
    "    #unbalanced data so allow prediction with given threshold\n",
    "    def predict_threshold(self, X_test, threshold):\n",
    "        Ret = np.zeros((X_test.shape[0],len(self.genre_dict.keys())), dtype='int')\n",
    "        for category in self.genre_dict.keys():\n",
    "            try:\n",
    "                prediction = self.pipelines[category].predict_proba(X_test)[:,1]\n",
    "            except:#exception we get is it was trained with data taht was only 0 label during cross validation\n",
    "                prediction = np.zeros(X_test.shape[0], dtype=int)\n",
    "            prediction[prediction >=threshold] = 1\n",
    "            prediction[prediction < threshold] = 0\n",
    "            Ret[:,self.genre_dict[category]] = prediction\n",
    "        return Ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metric definitions and printing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of intersection of predicted and actual labels divided by size of their union for each datapoint tested on\n",
    "#sum those and then divide by number of datapoints\n",
    "#vectorized for speed\n",
    "def multi_label_accuracy(real_labels_matrix, predictions_labels_matrix):\n",
    "    #binary so set intersection is and operator\n",
    "    intersection = real_labels_matrix & predictions_labels_matrix\n",
    "    #set union for binary is same as or operator\n",
    "    union = real_labels_matrix | predictions_labels_matrix\n",
    "    #sum(array.T) gets number of 1s in row\n",
    "    row_wise_accuracy = sum(intersection.T) / sum(union.T)\n",
    "    return sum(row_wise_accuracy) / real_labels_matrix.shape[0]\n",
    "\n",
    "#size of intersection of predicted and actual labels divided by size of predicted set for each datapoint tested on\n",
    "#sum those and divide by number of datapoints\n",
    "#if no predicted labels, don't count that row towards the precision as that would be undefined\n",
    "def multi_label_precision(real_labels_matrix, predictions_labels_matrix):\n",
    "    #binary so set intersection is and operator\n",
    "    intersection = real_labels_matrix & predictions_labels_matrix\n",
    "    precision_sum = 0\n",
    "    num_rows = 0\n",
    "    for row in range(intersection.shape[0]):\n",
    "        if sum(predictions_labels_matrix[row]) > 0: #if there is at least one prediction for this row\n",
    "            num_rows += 1\n",
    "            precision_sum += sum(intersection[row]) / sum(predictions_labels_matrix[row])\n",
    "    if num_rows == 0:\n",
    "        return 0#no labels predicted at all will give us 0 precision as precision makes no sense here\n",
    "    return precision_sum / num_rows\n",
    "\n",
    "#size of intersection of predicted and actual labels divided by size of real label set for each datapoint tested on\n",
    "#sum those and divide by number of datapoints\n",
    "#all datapoints should have at least 1 real label in this data set\n",
    "#vectorized for speed\n",
    "def multi_label_recall(real_labels_matrix, predictions_labels_matrix):\n",
    "    #binary so set intersection is and operator\n",
    "    intersection = real_labels_matrix & predictions_labels_matrix\n",
    "    #set union for binary is same as or operator\n",
    "    #sum(array.T) gets number of 1s in row\n",
    "    row_wise_recall = sum(intersection.T) / sum(real_labels_matrix.T)\n",
    "    return sum(row_wise_recall) / real_labels_matrix.shape[0]\n",
    "\n",
    "#lower is better\n",
    "def hamming_loss(real_labels_matrix, predictions_labels_matrix):\n",
    "    return (np.logical_xor(real_labels_matrix, predictions_labels_matrix)).sum()/(real_labels_matrix.shape[0] * real_labels_matrix.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_label_metrics(real_labels_matrix, predictions_labels_matrix):\n",
    "    for genre in genre_dict.keys():\n",
    "        index = genre_dict[genre]\n",
    "        real_labels_vect = real_labels_matrix[:, index]\n",
    "        prediction_vect = predictions_labels_matrix[:,index]\n",
    "        print(\"Accuruacy for \" + genre + \": \" + str(accuracy_score(real_labels_vect, prediction_vect)))\n",
    "        print(\"Precision for \" + genre + \": \" + str(precision_score(real_labels_vect, prediction_vect)))\n",
    "        print(\"Recall for \" + genre + \": \" + str(recall_score(real_labels_vect, prediction_vect)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(actual_labels, predictions):\n",
    "    print('Getting evaluation metrics for each label:')\n",
    "    get_per_label_metrics(actual_labels, predictions)\n",
    "    print('Getting evaluations for multilabel problem')\n",
    "    print('Multilabel accuracy: ' + str(multi_label_accuracy(actual_labels, predictions)))\n",
    "    print('Multilabel precision: ' + str(multi_label_precision(actual_labels, predictions)))\n",
    "    print('Multilabel recall: ' + str(multi_label_recall(actual_labels, predictions)))\n",
    "    print(\"Percent of correctly decided label decisions: \" + str(100* (1-hamming_loss(test_targets_arr, predictions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = MultiLabelLogisitcRegression(genre_dict)\n",
    "multi.fit(X_train, train_targets_arr)\n",
    "results = multi.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_metrics(test_targets_arr, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the poor multilabel metrics despite high accuracy on each label when considered alone. Do cross validation to find better threshold than .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k fold cross validation with threshold\n",
    "def kFoldCrossValidation(X, y, folds, threshold):\n",
    "    print(\"Doing cross validation for threshold = \" + str(threshold))\n",
    "    held_out_size = len(X)//folds\n",
    "    multi_label_acc = 0\n",
    "    for i in range(folds):\n",
    "        print(\"Iteration \" + str(i+1) + \" of \" + str(folds) + \" fold cross validation\")\n",
    "        held_out_index = i*held_out_size\n",
    "        if i == folds-1:\n",
    "            held_out_data = X[held_out_index:]\n",
    "            held_out_y = y[held_out_index:]\n",
    "            iter_training_data = X[0:held_out_index]\n",
    "            iter_y = y[0:held_out_index]\n",
    "        else:\n",
    "            held_out_data = X[held_out_index:held_out_index+held_out_size]\n",
    "            held_out_y = y[held_out_index:held_out_index+held_out_size]\n",
    "            iter_training_data = np.append(X[0:held_out_index], X[held_out_index+held_out_size:], axis=0)\n",
    "            iter_y = np.append(y[0:held_out_index], y[held_out_index+held_out_size:], axis=0)\n",
    "        multi = MultiLabelLogisitcRegression(genre_dict)\n",
    "        multi.fit(iter_training_data, iter_y)\n",
    "        predictions = multi.predict_threshold(held_out_data, threshold)\n",
    "        multi_label_acc += multi_label_accuracy(held_out_y, predictions)\n",
    "    return multi_label_acc / folds #sum accross all folds and divide by number of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best threshold looking from .3 to .7 in ntervals of .05\n",
    "test_threshold = .3\n",
    "best_threshold_acc = 0\n",
    "best_threshold = .5 #default is .5\n",
    "start = time.time()\n",
    "while test_threshold <= .7:\n",
    "    acc = kFoldCrossValidation(X_train, train_targets_arr, 5, test_threshold)\n",
    "    print(\"Accuracy: \" + str(acc))\n",
    "    if acc > best_threshold_acc:\n",
    "        best_threshold_acc = acc\n",
    "        best_threshold = test_threshold\n",
    "    test_threshold += .05\n",
    "end = time.time()\n",
    "print('Time to run k fold cross validation to find best threshold ' + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best threshold for multilabel accuracy: ' + str(best_threshold))\n",
    "classifier = MultiLabelLogisitcRegression(genre_dict)\n",
    "classifier.fit(X_train, train_targets_arr)\n",
    "predictions = multi.predict_threshold(X_test, best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_metrics(test_targets_arr, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
